{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3TyWDuJhHrm",
        "outputId": "6ec10146-4e22-42c1-b19c-a9d5cc1cb44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m793.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q cassio datasets langchain openai tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "#to initialize a db connection\n",
        "import cassio\n"
      ],
      "metadata": {
        "id": "gLqu12sWizPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcXcyGiZjnUF",
        "outputId": "7a86f929-360a-4b07-c750-253514253c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.7/232.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "ONRI_2hJj0R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ASTRA_TOKEN = \"ASTRA TOKEN\"\n",
        "VECTOR_DB_ID= \"VECTOR DB ID\"\n",
        "OPENAI_TOKEN= \"OPEN AI KEY\""
      ],
      "metadata": {
        "id": "PZXqdUMAj6nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_pdf = PdfReader(\"research_paper.pdf\")\n",
        "read_pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgBgFjQdmYuJ",
        "outputId": "4207518c-da1a-4045-fc17-61586126207d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PyPDF2._reader.PdfReader at 0x7df395d1dba0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Concatenate\n",
        "pdf_text = \" \"\n",
        "for i,page in enumerate(read_pdf.pages):\n",
        "  content = page.extract_text()\n",
        "  if content:\n",
        "    pdf_text+= content\n",
        "pdf_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "XO0MnR8immkz",
        "outputId": "cbb3ff38-2484-4f94-89ba-fcb812bde7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Neural Collaborative Filtering\\x03\\nXiangnan He\\nNational University of\\nSingapore, Singapore\\nxiangnanhe@gmail.comLizi Liao\\nNational University of\\nSingapore, Singapore\\nliaolizi.llz@gmail.comHanwang Zhang\\nColumbia University\\nUSA\\nhanwangzhang@gmail.com\\nLiqiang Nie\\nShandong University\\nChina\\nnieliqiang@gmail.comXia Hu\\nTexas A&M University\\nUSA\\nhu@cse.tamu.eduTat-Seng Chua\\nNational University of\\nSingapore, Singapore\\ndcscts@nus.edu.sg\\nABSTRACT\\nIn recent years, deep neural networks have yielded immense\\nsuccess on speech recognition, computer vision and natural\\nlanguage processing. However, the exploration of deep neu-\\nral networks on recommender systems has received relatively\\nless scrutiny. In this work, we strive to develop techniques\\nbased on neural networks to tackle the key problem in rec-\\nommendation | collaborative \\x0cltering | on the basis of\\nimplicit feedback.\\nAlthough some recent work has employed deep learning\\nfor recommendation, they primarily used it to model auxil-\\niary information, such as textual descriptions of items and\\nacoustic features of musics. When it comes to model the key\\nfactor in collaborative \\x0cltering | the interaction between\\nuser and item features, they still resorted to matrix factor-\\nization and applied an inner product on the latent features\\nof users and items.\\nBy replacing the inner product with a neural architecture\\nthat can learn an arbitrary function from data, we present\\na general framework named NCF, short for Neural network-\\nbased Collaborative Filtering . NCF is generic and can ex-\\npress and generalize matrix factorization under its frame-\\nwork. To supercharge NCF modelling with non-linearities,\\nwe propose to leverage a multi-layer perceptron to learn the\\nuser{item interaction function. Extensive experiments on\\ntwo real-world datasets show signi\\x0ccant improvements of our\\nproposed NCF framework over the state-of-the-art methods.\\nEmpirical evidence shows that using deeper layers of neural\\nnetworks o\\x0bers better recommendation performance.\\nKeywords\\nCollaborative Filtering, Neural Networks, Deep Learning,\\nMatrix Factorization, Implicit Feedback\\n\\x03NExT research is supported by the National Research\\nFoundation, Prime Minister\\'s O\\x0ece, Singapore under its\\nIRC@SG Funding Initiative.\\nc\\r2017 International World Wide Web Conference Committee\\n(IW3C2), published under Creative Commons CC BY 4.0 License.\\nWWW 2017, April 3â€“7, 2017, Perth, Australia.\\nACM 978-1-4503-4913-0/17/04.\\nhttp://dx.doi.org/10.1145/3038912.3052569\\n.1. INTRODUCTION\\nIn the era of information explosion, recommender systems\\nplay a pivotal role in alleviating information overload, hav-\\ning been widely adopted by many online services, including\\nE-commerce, online news and social media sites. The key to\\na personalized recommender system is in modelling users\\'\\npreference on items based on their past interactions ( e.g.,\\nratings and clicks), known as collaborative \\x0cltering [31, 46].\\nAmong the various collaborative \\x0cltering techniques, matrix\\nfactorization (MF) [14, 21] is the most popular one, which\\nprojects users and items into a shared latent space, using\\na vector of latent features to represent a user or an item.\\nThereafter a user\\'s interaction on an item is modelled as the\\ninner product of their latent vectors.\\nPopularized by the Net\\rix Prize, MF has become the de\\nfacto approach to latent factor model-based recommenda-\\ntion. Much research e\\x0bort has been devoted to enhancing\\nMF, such as integrating it with neighbor-based models [21],\\ncombining it with topic models of item content [38], and ex-\\ntending it to factorization machines [26] for a generic mod-\\nelling of features. Despite the e\\x0bectiveness of MF for collab-\\norative \\x0cltering, it is well-known that its performance can be\\nhindered by the simple choice of the interaction function |\\ninner product. For example, for the task of rating prediction\\non explicit feedback, it is well known that the performance\\nof the MF model can be improved by incorporating user\\nand item bias terms into the interaction function1. While\\nit seems to be just a trivial tweak for the inner product\\noperator [14], it points to the positive e\\x0bect of designing a\\nbetter, dedicated interaction function for modelling the la-\\ntent feature interactions between users and items. The inner\\nproduct, which simply combines the multiplication of latent\\nfeatures linearly, may not be su\\x0ecient to capture the com-\\nplex structure of user interaction data.\\nThis paper explores the use of deep neural networks for\\nlearning the interaction function from data, rather than a\\nhandcraft that has been done by many previous work [18,\\n21]. The neural network has been proven to be capable of\\napproximating any continuous function [17], and more re-\\ncently deep neural networks (DNNs) have been found to be\\ne\\x0bective in several domains, ranging from computer vision,\\nspeech recognition, to text processing [5, 10, 15, 47]. How-\\never, there is relatively little work on employing DNNs for\\nrecommendation in contrast to the vast amount of literature\\n1http://alex.smola.org/teaching/berkeley2012/slides/8_\\nRecommender.pdfarXiv:1708.05031v2  [cs.IR]  26 Aug 2017on MF methods. Although some recent advances [37, 38,\\n45] have applied DNNs to recommendation tasks and shown\\npromising results, they mostly used DNNs to model auxil-\\niary information, such as textual description of items, audio\\nfeatures of musics, and visual content of images. With re-\\ngards to modelling the key collaborative \\x0cltering e\\x0bect, they\\nstill resorted to MF, combining user and item latent features\\nusing an inner product.\\nThis work addresses the aforementioned research prob-\\nlems by formalizing a neural network modelling approach for\\ncollaborative \\x0cltering. We focus on implicit feedback, which\\nindirectly re\\rects users\\' preference through behaviours like\\nwatching videos, purchasing products and clicking items.\\nCompared to explicit feedback ( i.e., ratings and reviews),\\nimplicit feedback can be tracked automatically and is thus\\nmuch easier to collect for content providers. However, it is\\nmore challenging to utilize, since user satisfaction is not ob-\\nserved and there is a natural scarcity of negative feedback.\\nIn this paper, we explore the central theme of how to utilize\\nDNNs to model noisy implicit feedback signals.\\nThe main contributions of this work are as follows.\\n1. We present a neural network architecture to model\\nlatent features of users and items and devise a gen-\\neral framework NCF for collaborative \\x0cltering based\\non neural networks.\\n2. We show that MF can be interpreted as a specialization\\nof NCF and utilize a multi-layer perceptron to endow\\nNCF modelling with a high level of non-linearities.\\n3. We perform extensive experiments on two real-world\\ndatasets to demonstrate the e\\x0bectiveness of our NCF\\napproaches and the promise of deep learning for col-\\nlaborative \\x0cltering.\\n2. PRELIMINARIES\\nWe \\x0crst formalize the problem and discuss existing solu-\\ntions for collaborative \\x0cltering with implicit feedback. We\\nthen shortly recapitulate the widely used MF model, high-\\nlighting its limitation caused by using an inner product.\\n2.1 Learning from Implicit Data\\nLetMandNdenote the number of users and items,\\nrespectively. We de\\x0cne the user{item interaction matrix\\nY2RM\\x02Nfrom users\\' implicit feedback as,\\nyui=(\\n1;if interaction (user u, itemi) is observed;\\n0;otherwise.(1)\\nHere a value of 1 for yuiindicates that there is an interac-\\ntion between user uand itemi; however, it does not mean u\\nactually likes i. Similarly, a value of 0 does not necessarily\\nmeanudoes not like i, it can be that the user is not aware\\nof the item. This poses challenges in learning from implicit\\ndata, since it provides only noisy signals about users\\' pref-\\nerence. While observed entries at least re\\rect users\\' interest\\non items, the unobserved entries can be just missing data\\nand there is a natural scarcity of negative feedback.\\nThe recommendation problem with implicit feedback is\\nformulated as the problem of estimating the scores of unob-\\nserved entries in Y, which are used for ranking the items.\\nModel-based approaches assume that data can be generated\\n(or described) by an underlying model. Formally, they can\\nbe abstracted as learning ^ yui=f(u;ij\\x02);where ^yuidenotes\\nu1\\nu2\\nu3\\nu4i1i2i3i4i5\\n11101\\n01100\\n01110\\n10111\\nitems \\nusers (a) user{item matrix\\np1\\np2\\np3p4p\\'4 (b) user latent space\\nFigure 1: An example illustrates MF\\'s limitation.\\nFrom data matrix (a), u4is most similar to u1, fol-\\nlowed by u3, and lastly u2. However in the latent\\nspace (b), placing p4closest to p1makes p4closer to\\np2than p3, incurring a large ranking loss.\\nthe predicted score of interaction yui, \\x02 denotes model pa-\\nrameters, and fdenotes the function that maps model pa-\\nrameters to the predicted score (which we term as an inter-\\naction function ).\\nTo estimate parameters \\x02, existing approaches generally\\nfollow the machine learning paradigm that optimizes an ob-\\njective function. Two types of objective functions are most\\ncommonly used in literature | pointwise loss [14, 19] and\\npairwise loss [27, 33]. As a natural extension of abundant\\nwork on explicit feedback [21, 46], methods on pointwise\\nlearning usually follow a regression framework by minimiz-\\ning the squared loss between ^ yuiand its target value yui.\\nTo handle the absence of negative data, they have either\\ntreated all unobserved entries as negative feedback, or sam-\\npled negative instances from unobserved entries [14]. For\\npairwise learning [27, 44], the idea is that observed entries\\nshould be ranked higher than the unobserved ones. As such,\\ninstead of minimizing the loss between ^ yuiandyui, pairwise\\nlearning maximizes the margin between observed entry ^ yui\\nand unobserved entry ^ yuj.\\nMoving one step forward, our NCF framework parame-\\nterizes the interaction function fusing neural networks to\\nestimate ^yui. As such, it naturally supports both pointwise\\nand pairwise learning.\\n2.2 Matrix Factorization\\nMF associates each user and item with a real-valued vector\\nof latent features. Let puandqidenote the latent vector for\\nuseruand itemi, respectively; MF estimates an interaction\\nyuias the inner product of puandqi:\\n^yui=f(u;ijpu;qi) =pT\\nuqi=KX\\nk=1pukqik; (2)\\nwhereKdenotes the dimension of the latent space. As we\\ncan see, MF models the two-way interaction of user and item\\nlatent factors, assuming each dimension of the latent space\\nis independent of each other and linearly combining them\\nwith the same weight. As such, MF can be deemed as a\\nlinear model of latent factors.\\nFigure 1 illustrates how the inner product function can\\nlimit the expressiveness of MF. There are two settings to be\\nstated clearly beforehand to understand the example well.\\nFirst, since MF maps users and items to the same latent\\nspace, the similarity between two users can also be measured\\nwith an inner product, or equivalently2, the cosine of the\\nangle between their latent vectors. Second, without loss of\\n2Assuming latent vectors are of a unit length.Input Layer (Sparse)Embedding LayerNeural CF LayersOutput Layer\\n1000 00â€¦â€¦\\nUser ( u)0000 10â€¦â€¦\\nItem ( i)User Latent Vector Item Latent VectorLayer 1Layer 2Layer X\\nâ€¦â€¦Score TargetTrainingÅ·uiyui\\nPMÃ—K= {puk} QNÃ—K= {qik}Figure 2: Neural collaborative \\x0cltering framework\\ngenerality, we use the Jaccard coe\\x0ecient3as the ground-\\ntruth similarity of two users that MF needs to recover.\\nLet us \\x0crst focus on the \\x0crst three rows (users) in Fig-\\nure 1a. It is easy to have s23(0:66)> s 12(0:5)> s 13(0:4).\\nAs such, the geometric relations of p1;p2;andp3in the la-\\ntent space can be plotted as in Figure 1b. Now, let us con-\\nsider a new user u4, whose input is given as the dashed line\\nin Figure 1a. We can have s41(0:6)> s 43(0:4)> s 42(0:2),\\nmeaning that u4is most similar to u1, followed by u3, and\\nlastlyu2. However, if a MF model places p4closest to p1\\n(the two options are shown in Figure 1b with dashed lines),\\nit will result in p4closer to p2thanp3, which unfortunately\\nwill incur a large ranking loss.\\nThe above example shows the possible limitation of MF\\ncaused by the use of a simple and \\x0cxed inner product to esti-\\nmate complex user{item interactions in the low-dimensional\\nlatent space. We note that one way to resolve the issue is\\nto use a large number of latent factors K. However, it may\\nadversely hurt the generalization of the model ( e.g., over-\\n\\x0ctting the data), especially in sparse settings [26]. In this\\nwork, we address the limitation by learning the interaction\\nfunction using DNNs from data.\\n3. NEURAL COLLABORATIVE FILTERING\\nWe \\x0crst present the general NCF framework, elaborat-\\ning how to learn NCF with a probabilistic model that em-\\nphasizes the binary property of implicit data. We then\\nshow that MF can be expressed and generalized under NCF.\\nTo explore DNNs for collaborative \\x0cltering, we then pro-\\npose an instantiation of NCF, using a multi-layer perceptron\\n(MLP) to learn the user{item interaction function. Lastly,\\nwe present a new neural matrix factorization model, which\\nensembles MF and MLP under the NCF framework; it uni-\\n\\x0ces the strengths of linearity of MF and non-linearity of\\nMLP for modelling the user{item latent structures.\\n3.1 General Framework\\nTo permit a full neural treatment of collaborative \\x0cltering,\\nwe adopt a multi-layer representation to model a user{item\\ninteraction yuias shown in Figure 2, where the output of one\\nlayer serves as the input of the next one. The bottom input\\nlayer consists of two feature vectors vU\\nuandvI\\nithat describe\\nuseruand itemi, respectively; they can be customized to\\nsupport a wide range of modelling of users and items, such\\n3LetRube the set of items that user uhas interacted with,\\nthen the Jaccard similarity of users iandjis de\\x0cned as\\nsij=jRij\\\\jR jj\\njRij[jR jj.as context-aware [28, 1], content-based [3], and neighbor-\\nbased [26]. Since this work focuses on the pure collaborative\\n\\x0cltering setting, we use only the identity of a user and an\\nitem as the input feature, transforming it to a binarized\\nsparse vector with one-hot encoding. Note that with such a\\ngeneric feature representation for inputs, our method can be\\neasily adjusted to address the cold-start problem by using\\ncontent features to represent users and items.\\nAbove the input layer is the embedding layer; it is a fully\\nconnected layer that projects the sparse representation to\\na dense vector. The obtained user (item) embedding can\\nbe seen as the latent vector for user (item) in the context\\nof latent factor model. The user embedding and item em-\\nbedding are then fed into a multi-layer neural architecture,\\nwhich we term as neural collaborative \\x0cltering layers , to map\\nthe latent vectors to prediction scores. Each layer of the neu-\\nral CF layers can be customized to discover certain latent\\nstructures of user{item interactions. The dimension of the\\nlast hidden layer Xdetermines the model\\'s capability. The\\n\\x0cnal output layer is the predicted score ^ yui, and training\\nis performed by minimizing the pointwise loss between ^ yui\\nand its target value yui. We note that another way to train\\nthe model is by performing pairwise learning, such as using\\nthe Bayesian Personalized Ranking [27] and margin-based\\nloss [33]. As the focus of the paper is on the neural network\\nmodelling part, we leave the extension to pairwise learning\\nof NCF as a future work.\\nWe now formulate the NCF\\'s predictive model as\\n^yui=f(PTvU\\nu;QTvI\\nijP;Q;\\x02f); (3)\\nwhere P2RM\\x02KandQ2RN\\x02K, denoting the latent fac-\\ntor matrix for users and items, respectively; and \\x02 fdenotes\\nthe model parameters of the interaction function f. Since\\nthe function fis de\\x0cned as a multi-layer neural network, it\\ncan be formulated as\\nf(PTvU\\nu;QTvI\\ni) =\\x1eout(\\x1eX(:::\\x1e 2(\\x1e1(PTvU\\nu;QTvI\\ni)):::));\\n(4)\\nwhere\\x1eoutand\\x1exrespectively denote the mapping function\\nfor the output layer and x-th neural collaborative \\x0cltering\\n(CF) layer, and there are Xneural CF layers in total.\\n3.1.1 Learning NCF\\nTo learn model parameters, existing pointwise methods [14,\\n39] largely perform a regression with squared loss:\\nLsqr=X\\n(u;i)2Y[Y\\x00wui(yui\\x00^yui)2; (5)\\nwhereYdenotes the set of observed interactions in Y, and\\nY\\x00denotes the set of negative instances, which can be all (or\\nsampled from) unobserved interactions; and wuiis a hyper-\\nparameter denoting the weight of training instance ( u;i).\\nWhile the squared loss can be explained by assuming that\\nobservations are generated from a Gaussian distribution [29],\\nwe point out that it may not tally well with implicit data.\\nThis is because for implicit data, the target value yuiis\\na binarized 1 or 0 denoting whether uhas interacted with\\ni. In what follows, we present a probabilistic approach for\\nlearning the pointwise NCF that pays special attention to\\nthe binary property of implicit data.\\nConsidering the one-class nature of implicit feedback, we\\ncan view the value of yuias a label | 1 means item iis\\nrelevant to u, and 0 otherwise. The prediction score ^ yuithen represents how likely iis relevant to u. To endow NCF\\nwith such a probabilistic explanation, we need to constrain\\nthe output ^ yuiin the range of [0 ;1], which can be easily\\nachieved by using a probabilistic function ( e.g., theLogistic\\norProbit function) as the activation function for the output\\nlayer\\x1eout. With the above settings, we then de\\x0cne the\\nlikelihood function as\\np(Y;Y\\x00jP;Q;\\x02f) =Y\\n(u;i)2Y^yuiY\\n(u;j)2Y\\x00(1\\x00^yuj):(6)\\nTaking the negative logarithm of the likelihood, we reach\\nL=\\x00X\\n(u;i)2Ylog ^yui\\x00X\\n(u;j)2Y\\x00log(1\\x00^yuj)\\n=\\x00X\\n(u;i)2Y[Y\\x00yuilog ^yui+ (1\\x00yui) log(1\\x00^yui):(7)\\nThis is the objective function to minimize for the NCF meth-\\nods, and its optimization can be done by performing stochas-\\ntic gradient descent (SGD). Careful readers might have real-\\nized that it is the same as the binary cross-entropy loss , also\\nknown as log loss . By employing a probabilistic treatment\\nfor NCF, we address recommendation with implicit feedback\\nas a binary classi\\x0ccation problem. As the classi\\x0ccation-\\naware log loss has rarely been investigated in recommen-\\ndation literature, we explore it in this work and empirically\\nshow its e\\x0bectiveness in Section 4.3. For the negative in-\\nstancesY\\x00, we uniformly sample them from unobserved in-\\nteractions in each iteration and control the sampling ratio\\nw.r.t. the number of observed interactions. While a non-\\nuniform sampling strategy ( e.g., item popularity-biased [14,\\n12]) might further improve the performance, we leave the\\nexploration as a future work.\\n3.2 Generalized Matrix Factorization (GMF)\\nWe now show how MF can be interpreted as a special case\\nof our NCF framework. As MF is the most popular model\\nfor recommendation and has been investigated extensively\\nin literature, being able to recover it allows NCF to mimic\\na large family of factorization models [26].\\nDue to the one-hot encoding of user (item) ID of the input\\nlayer, the obtained embedding vector can be seen as the\\nlatent vector of user (item). Let the user latent vector pu\\nbePTvU\\nuand item latent vector qibeQTvI\\ni. We de\\x0cne the\\nmapping function of the \\x0crst neural CF layer as\\n\\x1e1(pu;qi) =pu\\x0cqi; (8)\\nwhere\\x0cdenotes the element-wise product of vectors. We\\nthen project the vector to the output layer:\\n^yui=aout(hT(pu\\x0cqi)); (9)\\nwhereaoutandhdenote the activation function and edge\\nweights of the output layer, respectively. Intuitively, if we\\nuse an identity function for aoutand enforce hto be a uni-\\nform vector of 1, we can exactly recover the MF model.\\nUnder the NCF framework, MF can be easily general-\\nized and extended. For example, if we allow hto be learnt\\nfrom data without the uniform constraint, it will result in\\na variant of MF that allows varying importance of latent\\ndimensions. And if we use a non-linear function for aout, it\\nwill generalize MF to a non-linear setting which might be\\nmore expressive than the linear MF model. In this work, we\\nimplement a generalized version of MF under NCF that usesthe sigmoid function \\x1b(x) = 1=(1 +e\\x00x) asaoutand learns\\nhfrom data with the log loss (Section 3.1.1). We term it as\\nGMF, short for Generalized Matrix Factorization .\\n3.3 Multi-Layer Perceptron (MLP)\\nSince NCF adopts two pathways to model users and items,\\nit is intuitive to combine the features of two pathways by\\nconcatenating them. This design has been widely adopted\\nin multimodal deep learning work [47, 34]. However, simply\\na vector concatenation does not account for any interactions\\nbetween user and item latent features, which is insu\\x0ecient\\nfor modelling the collaborative \\x0cltering e\\x0bect. To address\\nthis issue, we propose to add hidden layers on the concate-\\nnated vector, using a standard MLP to learn the interaction\\nbetween user and item latent features. In this sense, we can\\nendow the model a large level of \\rexibility and non-linearity\\nto learn the interactions between puandqi, rather than the\\nway of GMF that uses only a \\x0cxed element-wise product\\non them. More precisely, the MLP model under our NCF\\nframework is de\\x0cned as\\nz1=\\x1e1(pu;qi) =\\x14\\npu\\nqi\\x15\\n;\\n\\x1e2(z1) =a2(WT\\n2z1+b2);\\n::::::\\n\\x1eL(zL\\x001) =aL(WT\\nLzL\\x001+bL);\\n^yui=\\x1b(hT\\x1eL(zL\\x001));(10)\\nwhere Wx,bx, andaxdenote the weight matrix, bias vec-\\ntor, and activation function for the x-th layer\\'s perceptron,\\nrespectively. For activation functions of MLP layers, one\\ncan freely choose sigmoid, hyperbolic tangent (tanh), and\\nRecti\\x0cer (ReLU), among others. We would like to ana-\\nlyze each function: 1) The sigmoid function restricts each\\nneuron to be in (0,1), which may limit the model\\'s perfor-\\nmance; and it is known to su\\x0ber from saturation, where\\nneurons stop learning when their output is near either 0 or\\n1. 2) Even though tanh is a better choice and has been\\nwidely adopted [6, 44], it only alleviates the issues of sig-\\nmoid to a certain extent, since it can be seen as a rescaled\\nversion of sigmoid (tanh( x=2) = 2\\x1b(x)\\x001). And 3) as\\nsuch, we opt for ReLU, which is more biologically plausi-\\nble and proven to be non-saturated [9]; moreover, it encour-\\nages sparse activations, being well-suited for sparse data and\\nmaking the model less likely to be over\\x0ctting. Our empirical\\nresults show that ReLU yields slightly better performance\\nthan tanh, which in turn is signi\\x0ccantly better than sigmoid.\\nAs for the design of network structure, a common solution\\nis to follow a tower pattern, where the bottom layer is the\\nwidest and each successive layer has a smaller number of\\nneurons (as in Figure 2). The premise is that by using a\\nsmall number of hidden units for higher layers, they can\\nlearn more abstractive features of data [10]. We empirically\\nimplement the tower structure, halving the layer size for\\neach successive higher layer.\\n3.4 Fusion of GMF and MLP\\nSo far we have developed two instantiations of NCF |\\nGMF that applies a linear kernel to model the latent feature\\ninteractions, and MLP that uses a non-linear kernel to learn\\nthe interaction function from data. The question then arises:\\nhow can we fuse GMF and MLP under the NCF framework,1000 00â€¦â€¦\\nUser ( u)0000 10â€¦â€¦\\nItem ( i)MF User Vector MF Item VectorGMF Layer â€¦â€¦Score TargetTrainingÅ·uiyui\\nMLP Layer 1\\nMLP User Vector MLP Item VectorElement -wise \\nProduct\\nConcatenationMLP Layer 2MLP Layer XNeuMF LayerLog loss\\nğˆ\\nReLUReLUConcatenationFigure 3: Neural matrix factorization model\\nso that they can mutually reinforce each other to better\\nmodel the complex user-iterm interactions?\\nA straightforward solution is to let GMF and MLP share\\nthe same embedding layer, and then combine the outputs of\\ntheir interaction functions. This way shares a similar spirit\\nwith the well-known Neural Tensor Network (NTN) [33].\\nSpeci\\x0ccally, the model for combining GMF with a one-layer\\nMLP can be formulated as\\n^yui=\\x1b(hTa(pu\\x0cqi+W\\x14\\npu\\nqi\\x15\\n+b)): (11)\\nHowever, sharing embeddings of GMF and MLP might\\nlimit the performance of the fused model. For example,\\nit implies that GMF and MLP must use the same size of\\nembeddings; for datasets where the optimal embedding size\\nof the two models varies a lot, this solution may fail to obtain\\nthe optimal ensemble.\\nTo provide more \\rexibility to the fused model, we allow\\nGMF and MLP to learn separate embeddings, and combine\\nthe two models by concatenating their last hidden layer.\\nFigure 3 illustrates our proposal, the formulation of which\\nis given as follows\\n\\x1eGMF=pG\\nu\\x0cqG\\ni;\\n\\x1eMLP=aL(WT\\nL(aL\\x001(:::a2(WT\\n2\\x14\\npM\\nu\\nqM\\ni\\x15\\n+b2):::)) +bL);\\n^yui=\\x1b(hT\\x14\\n\\x1eGMF\\n\\x1eMLP\\x15\\n);\\n(12)\\nwhere pG\\nuandpM\\nudenote the user embedding for GMF\\nand MLP parts, respectively; and similar notations of qG\\ni\\nandqM\\nifor item embeddings. As discussed before, we use\\nReLU as the activation function of MLP layers. This model\\ncombines the linearity of MF and non-linearity of DNNs for\\nmodelling user{item latent structures. We dub this model\\n\\\\NeuMF\", short for Neural Matrix Factorization . The deriva-\\ntive of the model w.r.t. each model parameter can be cal-\\nculated with standard back-propagation, which is omitted\\nhere due to space limitation.\\n3.4.1 Pre-training\\nDue to the non-convexity of the objective function of NeuMF,\\ngradient-based optimization methods only \\x0cnd locally-optimal\\nsolutions. It is reported that the initialization plays an im-\\nportant role for the convergence and performance of deep\\nlearning models [7]. Since NeuMF is an ensemble of GMFand MLP, we propose to initialize NeuMF using the pre-\\ntrained models of GMF and MLP.\\nWe \\x0crst train GMF and MLP with random initializations\\nuntil convergence. We then use their model parameters as\\nthe initialization for the corresponding parts of NeuMF\\'s\\nparameters. The only tweak is on the output layer, where\\nwe concatenate weights of the two models with\\nh \\x14\\n\\x0bhGMF\\n(1\\x00\\x0b)hMLP\\x15\\n; (13)\\nwhere hGMFandhMLPdenote the hvector of the pre-\\ntrained GMF and MLP model, respectively; and \\x0bis a\\nhyper-parameter determining the trade-o\\x0b between the two\\npre-trained models.\\nFor training GMF and MLP from scratch, we adopt the\\nAdaptive Moment Estimation (Adam) [20], which adapts\\nthe learning rate for each parameter by performing smaller\\nupdates for frequent and larger updates for infrequent pa-\\nrameters. The Adam method yields faster convergence for\\nboth models than the vanilla SGD and relieves the pain of\\ntuning the learning rate. After feeding pre-trained parame-\\nters into NeuMF, we optimize it with the vanilla SGD, rather\\nthan Adam. This is because Adam needs to save momentum\\ninformation for updating parameters properly. As we ini-\\ntialize NeuMF with pre-trained model parameters only and\\nforgo saving the momentum information, it is unsuitable to\\nfurther optimize NeuMF with momentum-based methods.\\n4. EXPERIMENTS\\nIn this section, we conduct experiments with the aim of\\nanswering the following research questions:\\nRQ1 Do our proposed NCF methods outperform the state-\\nof-the-art implicit collaborative \\x0cltering methods?\\nRQ2 How does our proposed optimization framework (log\\nloss with negative sampling) work for the recommen-\\ndation task?\\nRQ3 Are deeper layers of hidden units helpful for learning\\nfrom user{item interaction data?\\nIn what follows, we \\x0crst present the experimental settings,\\nfollowed by answering the above three research questions.\\n4.1 Experimental Settings\\nDatasets. We experimented with two publicly accessible\\ndatasets: MovieLens4and Pinterest5. The characteristics of\\nthe two datasets are summarized in Table 1.\\n1. MovieLens . This movie rating dataset has been\\nwidely used to evaluate collaborative \\x0cltering algorithms.\\nWe used the version containing one million ratings, where\\neach user has at least 20 ratings. While it is an explicit\\nfeedback data, we have intentionally chosen it to investigate\\nthe performance of learning from the implicit signal [21] of\\nexplicit feedback. To this end, we transformed it into im-\\nplicit data, where each entry is marked as 0 or 1 indicating\\nwhether the user has rated the item.\\n2. Pinterest . This implicit feedback data is constructed\\nby [8] for evaluating content-based image recommendation.\\n4http://grouplens.org/datasets/movielens/1m/\\n5https://sites.google.com/site/xueatalphabeta/\\nacademic-projectsTable 1: Statistics of the evaluation datasets.\\nDataset Interaction# Item# User# Sparsity\\nMovieLens 1,000,209 3,706 6,040 95.53%\\nPinterest 1,500,809 9,916 55,187 99.73%\\nThe original data is very large but highly sparse. For exam-\\nple, over 20% of users have only one pin, making it di\\x0ecult\\nto evaluate collaborative \\x0cltering algorithms. As such, we\\n\\x0cltered the dataset in the same way as the MovieLens data\\nthat retained only users with at least 20 interactions (pins).\\nThis results in a subset of the data that contains 55 ;187\\nusers and 1 ;500;809 interactions. Each interaction denotes\\nwhether the user has pinned the image to her own board.\\nEvaluation Protocols. To evaluate the performance of\\nitem recommendation, we adopted the leave-one-out evalu-\\nation, which has been widely used in literature [1, 14, 27].\\nFor each user, we held-out her latest interaction as the test\\nset and utilized the remaining data for training. Since it is\\ntoo time-consuming to rank all items for every user during\\nevaluation, we followed the common strategy [6, 21] that\\nrandomly samples 100 items that are not interacted by the\\nuser, ranking the test item among the 100 items. The perfor-\\nmance of a ranked list is judged by Hit Ratio (HR) and Nor-\\nmalized Discounted Cumulative Gain (NDCG) [11]. With-\\nout special mention, we truncated the ranked list at 10 for\\nboth metrics. As such, the HR intuitively measures whether\\nthe test item is present on the top-10 list, and the NDCG\\naccounts for the position of the hit by assigning higher scores\\nto hits at top ranks. We calculated both metrics for each\\ntest user and reported the average score.\\nBaselines. We compared our proposed NCF methods (GMF,\\nMLP and NeuMF) with the following methods:\\n-ItemPop . Items are ranked by their popularity judged\\nby the number of interactions. This is a non-personalized\\nmethod to benchmark the recommendation performance [27].\\n-ItemKNN [31]. This is the standard item-based col-\\nlaborative \\x0cltering method. We followed the setting of [19]\\nto adapt it for implicit data.\\n-BPR [27]. This method optimizes the MF model of\\nEquation 2 with a pairwise ranking loss, which is tailored\\nto learn from implicit feedback. It is a highly competitive\\nbaseline for item recommendation. We used a \\x0cxed learning\\nrate, varying it and reporting the best performance.\\n-eALS [14]. This is a state-of-the-art MF method for\\nitem recommendation. It optimizes the squared loss of Equa-\\ntion 5, treating all unobserved interactions as negative in-\\nstances and weighting them non-uniformly by the item pop-\\nularity. Since eALS shows superior performance over the\\nuniform-weighting method WMF [19], we do not further re-\\nport WMF\\'s performance.\\nAs our proposed methods aim to model the relationship\\nbetween users and items, we mainly compare with user{\\nitem models. We leave out the comparison with item{item\\nmodels, such as SLIM [25] and CDAE [44], because the per-\\nformance di\\x0berence may be caused by the user models for\\npersonalization (as they are item{item model).\\nParameter Settings. We implemented our proposed meth-\\nods based on Keras6. To determine hyper-parameters of\\nNCF methods, we randomly sampled one interaction for\\n6https://github.com/hexiangnan/neural_\\ncollaborative_filteringeach user as the validation data and tuned hyper-parameters\\non it. All NCF models are learnt by optimizing the log loss\\nof Equation 7, where we sampled four negative instances\\nper positive instance. For NCF models that are trained\\nfrom scratch, we randomly initialized model parameters with\\na Gaussian distribution (with a mean of 0 and standard\\ndeviation of 0 :01), optimizing the model with mini-batch\\nAdam [20]. We tested the batch size of [128 ;256;512;1024],\\nand the learning rate of [0.0001 ,0.0005, 0.001, 0.005]. Since\\nthe last hidden layer of NCF determines the model capa-\\nbility, we term it as predictive factors and evaluated the\\nfactors of [8 ;16;32;64]. It is worth noting that large factors\\nmay cause over\\x0ctting and degrade the performance. With-\\nout special mention, we employed three hidden layers for\\nMLP; for example, if the size of predictive factors is 8, then\\nthe architecture of the neural CF layers is 32 !16!8, and\\nthe embedding size is 16. For the NeuMF with pre-training,\\n\\x0bwas set to 0.5, allowing the pre-trained GMF and MLP to\\ncontribute equally to NeuMF\\'s initialization.\\n4.2 Performance Comparison (RQ1)\\nFigure 4 shows the performance of HR@10 and NDCG@10\\nwith respect to the number of predictive factors. For MF\\nmethods BPR and eALS, the number of predictive factors\\nis equal to the number of latent factors. For ItemKNN, we\\ntested di\\x0berent neighbor sizes and reported the best per-\\nformance. Due to the weak performance of ItemPop, it is\\nomitted in Figure 4 to better highlight the performance dif-\\nference of personalized methods.\\nFirst, we can see that NeuMF achieves the best perfor-\\nmance on both datasets, signi\\x0ccantly outperforming the state-\\nof-the-art methods eALS and BPR by a large margin (on\\naverage, the relative improvement over eALS and BPR is\\n4:5% and 4:9%, respectively). For Pinterest, even with a\\nsmall predictive factor of 8, NeuMF substantially outper-\\nforms that of eALS and BPR with a large factor of 64. This\\nindicates the high expressiveness of NeuMF by fusing the\\nlinear MF and non-linear MLP models. Second, the other\\ntwo NCF methods | GMF and MLP | also show quite\\nstrong performance. Between them, MLP slightly under-\\nperforms GMF. Note that MLP can be further improved by\\nadding more hidden layers (see Section 4.4), and here we\\nonly show the performance of three layers. For small pre-\\ndictive factors, GMF outperforms eALS on both datasets;\\nalthough GMF su\\x0bers from over\\x0ctting for large factors, its\\nbest performance obtained is better than (or on par with)\\nthat of eALS. Lastly, GMF shows consistent improvements\\nover BPR, admitting the e\\x0bectiveness of the classi\\x0ccation-\\naware log loss for the recommendation task, since GMF and\\nBPR learn the same MF model but with di\\x0berent objective\\nfunctions.\\nFigure 5 shows the performance of Top- Krecommended\\nlists where the ranking position Kranges from 1 to 10. To\\nmake the \\x0cgure more clear, we show the performance of\\nNeuMF rather than all three NCF methods. As can be\\nseen, NeuMF demonstrates consistent improvements over\\nother methods across positions, and we further conducted\\none-sample paired t-tests, verifying that all improvements\\nare statistically signi\\x0ccant for p<0:01. For baseline meth-\\nods, eALS outperforms BPR on MovieLens with about 5 :1%\\nrelative improvement, while underperforms BPR on Pinter-\\nest in terms of NDCG. This is consistent with [14]\\'s \\x0cnding\\nthat BPR can be a strong performer for ranking performance0.55 0.6 0.65 0.7 0.75 \\n8 16 32 64 HR@10 \\nFactors MovieLens \\nItemKNN BPR \\neALS GMF \\nMLP NeuMF (a) MovieLens | HR@10\\n0.3 0.34 0.38 0.42 0.46 \\n8 16 32 64 NDCG@10 \\nFactors MovieLens \\nItemKNN BPR \\neALS GMF \\nMLP NeuMF (b) MovieLens | NDCG@10\\n0.78 0.81 0.84 0.87 0.9 \\n8 16 32 64 HR@10 \\nFactors Pinterest \\nItemKNN BPR \\neALS GMF \\nMLP NeuMF (c) Pinterest | HR@10\\n0.48 0.5 0.52 0.54 0.56 \\n8 16 32 64 NDCG@10 \\nFactors Pinterest \\nItemKNN BPR \\neALS GMF \\nMLP NeuMF (d) Pinterest | NDCG@10\\nFigure 4: Performance of HR@10 and NDCG@10 w.r.t. the number of predictive factors on the two datasets.\\n0.1 0.25 0.4 0.55 0.7 \\n1 2 3 4 5 6 7 8 9 10 HR@K \\nKMovieLens \\nItemPop ItemKNN \\nBPR eALS \\nNeuMF \\n(a) MovieLens | HR@K\\n0.1 0.18 0.26 0.34 0.42 \\n1 2 3 4 5 6 7 8 9 10 NDCG@K \\nKMovieLens \\nItemPop ItemKNN \\nBPR eALS \\nNeuMF (b) MovieLens | NDCG@K\\n0.1 0.3 0.5 0.7 0.9 \\n1 2 3 4 5 6 7 8 9 10 HR@K \\nKPinterest \\nItemPop \\nItemKNN \\nBPR \\neALS \\nNeuMF (c) Pinterest | HR@K\\n0.1 0.22 0.34 0.46 0.58 \\n1 2 3 4 5 6 7 8 9 10 NDCG@K \\nKPinterest \\nItemPop \\nItemKNN \\nBPR \\neALS \\nNeuMF (d) Pinterest | NDCG@K\\nFigure 5: Evaluation of Top- Kitem recommendation where Kranges from 1to10on the two datasets.\\nowing to its pairwise ranking-aware learner. The neighbor-\\nbased ItemKNN underperforms model-based methods. And\\nItemPop performs the worst, indicating the necessity of mod-\\neling users\\' personalized preferences, rather than just recom-\\nmending popular items to users.\\n4.2.1 Utility of Pre-training\\nTo demonstrate the utility of pre-training for NeuMF, we\\ncompared the performance of two versions of NeuMF |\\nwith and without pre-training. For NeuMF without pre-\\ntraining, we used the Adam to learn it with random ini-\\ntializations. As shown in Table 2, the NeuMF with pre-\\ntraining achieves better performance in most cases; only\\nfor MovieLens with a small predictive factors of 8, the pre-\\ntraining method performs slightly worse. The relative im-\\nprovements of the NeuMF with pre-training are 2 :2% and\\n1:1% for MovieLens and Pinterest, respectively. This re-\\nsult justi\\x0ces the usefulness of our pre-training method for\\ninitializing NeuMF.\\nTable 2: Performance of NeuMF with and without\\npre-training.\\nWith Pre-training Without Pre-training\\nFactors HR@10 NDCG@10 HR@10 NDCG@10\\nMovieLens\\n8 0.684 0.403 0.688 0.410\\n16 0.707 0.426 0.696 0.420\\n32 0.726 0.445 0.701 0.425\\n64 0.730 0.447 0.705 0.426\\nPinterest\\n8 0.878 0.555 0.869 0.546\\n16 0.880 0.558 0.871 0.547\\n32 0.879 0.555 0.870 0.549\\n64 0.877 0.552 0.872 0.5514.3 Log Loss with Negative Sampling (RQ2)\\nTo deal with the one-class nature of implicit feedback,\\nwe cast recommendation as a binary classi\\x0ccation task. By\\nviewing NCF as a probabilistic model, we optimized it with\\nthe log loss. Figure 6 shows the training loss (averaged\\nover all instances) and recommendation performance of NCF\\nmethods of each iteration on MovieLens. Results on Pinter-\\nest show the same trend and thus they are omitted due to\\nspace limitation. First, we can see that with more iterations,\\nthe training loss of NCF models gradually decreases and\\nthe recommendation performance is improved. The most\\ne\\x0bective updates are occurred in the \\x0crst 10 iterations, and\\nmore iterations may over\\x0ct a model ( e.g., although the train-\\ning loss of NeuMF keeps decreasing after 10 iterations, its\\nrecommendation performance actually degrades). Second,\\namong the three NCF methods, NeuMF achieves the lowest\\ntraining loss, followed by MLP, and then GMF. The rec-\\nommendation performance also shows the same trend that\\nNeuMF>MLP>GMF. The above \\x0cndings provide empir-\\nical evidence for the rationality and e\\x0bectiveness of optimiz-\\ning the log loss for learning from implicit data.\\nAn advantage of pointwise log loss over pairwise objective\\nfunctions [27, 33] is the \\rexible sampling ratio for negative\\ninstances. While pairwise objective functions can pair only\\none sampled negative instance with a positive instance, we\\ncan \\rexibly control the sampling ratio of a pointwise loss. To\\nillustrate the impact of negative sampling for NCF methods,\\nwe show the performance of NCF methods w.r.t. di\\x0berent\\nnegative sampling ratios in Figure 7. It can be clearly seen\\nthat just one negative sample per positive instance is insuf-\\n\\x0ccient to achieve optimal performance, and sampling more\\nnegative instances is bene\\x0ccial. Comparing GMF to BPR,\\nwe can see the performance of GMF with a sampling ratio\\nof one is on par with BPR, while GMF signi\\x0ccantly betters0.1 0.2 0.3 0.4 0.5 \\n0 10 20 30 40 50 Training Loss \\nIteration MovieLens \\nGMF \\nMLP \\nNeuMF (a) Training Loss\\n0.1 0.3 0.5 0.7 \\n0 10 20 30 40 50 HR@10 \\nIteration MovieLens \\nGMF \\nMLP \\nNeuMF (b) HR@10\\n00.1 0.2 0.3 0.4 0.5 \\n0 10 20 30 40 50 NDCG@10 \\nIteration MovieLens \\nGMF \\nMLP \\nNeuMF (c) NDCG@10\\nFigure 6: Training loss and recommendation performance of NCF methods w.r.t. the number of iterations\\non MovieLens (factors=8).\\n0.62 0.64 0.66 0.68 0.7 0.72 \\n1 2 3 4 5 6 7 8 9 10 HR@10 \\nNumber of Negatives MovieLens \\nNeuMF \\nGMF \\nMLP \\nBPR \\n(a) MovieLens | HR@10\\n0.36 0.38 0.4 0.42 0.44 \\n1 2 3 4 5 6 7 8 9 10 NDCG@10 \\nNumber of Negatives MovieLens \\nNeuMF \\nGMF \\nMLP \\nBPR (b) MovieLens | NDCG@10\\n0.84 0.85 0.86 0.87 0.88 0.89 \\n1 2 3 4 5 6 7 8 9 10 HR@10 \\nNumber of Negatives Pinterest \\nNeuMF \\nGMF \\nMLP \\nBPR (c) Pinterest | HR@10\\n0.52 0.53 0.54 0.55 0.56 0.57 \\n1 2 3 4 5 6 7 8 9 10 NDCG@10 \\nNumber of Negatives Pinterest \\nNeuMF GMF \\nMLP BPR (d) Pinterest | NDCG@10\\nFigure 7: Performance of NCF methods w.r.t. the number of negative samples per positive instance (fac-\\ntors=16). The performance of BPR is also shown, which samples only one negative instance to pair with a\\npositive instance for learning.\\nBPR with larger sampling ratios. This shows the advan-\\ntage of pointwise log loss over the pairwise BPR loss. For\\nboth datasets, the optimal sampling ratio is around 3 to 6.\\nOn Pinterest, we \\x0cnd that when the sampling ratio is larger\\nthan 7, the performance of NCF methods starts to drop. It\\nreveals that setting the sampling ratio too aggressively may\\nadversely hurt the performance.\\n4.4 Is Deep Learning Helpful? (RQ3)\\nAs there is little work on learning user{item interaction\\nfunction with neural networks, it is curious to see whether\\nusing a deep network structure is bene\\x0ccial to the recom-\\nmendation task. Towards this end, we further investigated\\nMLP with di\\x0berent number of hidden layers. The results\\nare summarized in Table 3 and 4. The MLP-3 indicates\\nthe MLP method with three hidden layers (besides the em-\\nbedding layer), and similar notations for others. As we can\\nsee, even for models with the same capability, stacking more\\nlayers are bene\\x0ccial to performance. This result is highly\\nencouraging, indicating the e\\x0bectiveness of using deep mod-\\nels for collaborative recommendation. We attribute the im-\\nprovement to the high non-linearities brought by stacking\\nmore non-linear layers. To verify this, we further tried stack-\\ning linear layers, using an identity function as the activation\\nfunction. The performance is much worse than using the\\nReLU unit.\\nFor MLP-0 that has no hidden layers ( i.e.,the embedding\\nlayer is directly projected to predictions), the performance is\\nvery weak and is not better than the non-personalized Item-\\nPop. This veri\\x0ces our argument in Section 3.3 that simply\\nconcatenating user and item latent vectors is insu\\x0ecient for\\nmodelling their feature interactions, and thus the necessity\\nof transforming it with hidden layers.Table 3: HR@10 of MLP with di\\x0berent layers.\\nFactors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4\\nMovieLens\\n8 0.452 0.628 0.655 0.671 0.678\\n16 0.454 0.663 0.674 0.684 0.690\\n32 0.453 0.682 0.687 0.692 0.699\\n64 0.453 0.687 0.696 0.702 0.707\\nPinterest\\n8 0.275 0.848 0.855 0.859 0.862\\n16 0.274 0.855 0.861 0.865 0.867\\n32 0.273 0.861 0.863 0.868 0.867\\n64 0.274 0.864 0.867 0.869 0.873\\n5. RELATED WORK\\nWhile early literature on recommendation has largely fo-\\ncused on explicit feedback [30, 31], recent attention is in-\\ncreasingly shifting towards implicit data [1, 14, 23]. The\\ncollaborative \\x0cltering (CF) task with implicit feedback is\\nusually formulated as an item recommendation problem, for\\nwhich the aim is to recommend a short list of items to users.\\nIn contrast to rating prediction that has been widely solved\\nby work on explicit feedback, addressing the item recommen-\\ndation problem is more practical but challenging [1, 11]. One\\nkey insight is to model the missing data, which are always\\nignored by the work on explicit feedback [21, 48]. To tailor\\nlatent factor models for item recommendation with implicit\\nfeedback, early work [19, 27] applies a uniform weighting\\nwhere two strategies have been proposed | which either\\ntreated all missing data as negative instances [19] or sam-\\npled negative instances from missing data [27]. Recently, He\\net al. [14] and Liang et al. [23] proposed dedicated models\\nto weight missing data, and Rendle et al. [1] developed anTable 4: NDCG@10 of MLP with di\\x0berent layers.\\nFactors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4\\nMovieLens\\n8 0.253 0.359 0.383 0.399 0.406\\n16 0.252 0.391 0.402 0.410 0.415\\n32 0.252 0.406 0.410 0.425 0.423\\n64 0.251 0.409 0.417 0.426 0.432\\nPinterest\\n8 0.141 0.526 0.534 0.536 0.539\\n16 0.141 0.532 0.536 0.538 0.544\\n32 0.142 0.537 0.538 0.542 0.546\\n64 0.141 0.538 0.542 0.545 0.550\\nimplicit coordinate descent (iCD) solution for feature-based\\nfactorization models, achieving state-of-the-art performance\\nfor item recommendation. In the following, we discuss rec-\\nommendation works that use neural networks.\\nThe early pioneer work by Salakhutdinov et al. [30] pro-\\nposed a two-layer Restricted Boltzmann Machines (RBMs)\\nto model users\\' explicit ratings on items. The work was been\\nlater extended to model the ordinal nature of ratings [36].\\nRecently, autoencoders have become a popular choice for\\nbuilding recommendation systems [32, 22, 35]. The idea of\\nuser-based AutoRec [32] is to learn hidden structures that\\ncan reconstruct a user\\'s ratings given her historical ratings\\nas inputs. In terms of user personalization, this approach\\nshares a similar spirit as the item{item model [31, 25] that\\nrepresents a user as her rated items. To avoid autoencoders\\nlearning an identity function and failing to generalize to un-\\nseen data, denoising autoencoders (DAEs) have been applied\\nto learn from intentionally corrupted inputs [22, 35]. More\\nrecently, Zheng et al. [48] presented a neural autoregressive\\nmethod for CF. While the previous e\\x0bort has lent support\\nto the e\\x0bectiveness of neural networks for addressing CF,\\nmost of them focused on explicit ratings and modelled the\\nobserved data only. As a result, they can easily fail to learn\\nusers\\' preference from the positive-only implicit data.\\nAlthough some recent works [6, 37, 38, 43, 45] have ex-\\nplored deep learning models for recommendation based on\\nimplicit feedback, they primarily used DNNs for modelling\\nauxiliary information, such as textual description of items [38],\\nacoustic features of musics [37, 43], cross-domain behaviors\\nof users [6], and the rich information in knowledge bases [45].\\nThe features learnt by DNNs are then integrated with MF\\nfor CF. The work that is most relevant to our work is [44],\\nwhich presents a collaborative denoising autoencoder (CDAE)\\nfor CF with implicit feedback. In contrast to the DAE-based\\nCF [35], CDAE additionally plugs a user node to the input\\nof autoencoders for reconstructing the user\\'s ratings. As\\nshown by the authors, CDAE is equivalent to the SVD++\\nmodel [21] when the identity function is applied to acti-\\nvate the hidden layers of CDAE. This implies that although\\nCDAE is a neural modelling approach for CF, it still applies\\na linear kernel ( i.e.,inner product) to model user{item inter-\\nactions. This may partially explain why using deep layers for\\nCDAE does not improve the performance ( cf.Section 6 of\\n[44]). Distinct from CDAE, our NCF adopts a two-pathway\\narchitecture, modelling user{item interactions with a multi-\\nlayer feedforward neural network. This allows NCF to learn\\nan arbitrary function from the data, being more powerful\\nand expressive than the \\x0cxed inner product function.\\nAlong a similar line, learning the relations of two enti-\\nties has been intensively studied in literature of knowledgegraphs [2, 33]. Many relational machine learning methods\\nhave been devised [24]. The one that is most similar to our\\nproposal is the Neural Tensor Network (NTN) [33], which\\nuses neural networks to learn the interaction of two entities\\nand shows strong performance. Here we focus on a di\\x0ber-\\nent problem setting of CF. While the idea of NeuMF that\\ncombines MF with MLP is partially inspired by NTN, our\\nNeuMF is more \\rexible and generic than NTN, in terms of\\nallowing MF and MLP learning di\\x0berent sets of embeddings.\\nMore recently, Google publicized their Wide & Deep learn-\\ning approach for App recommendation [4]. The deep compo-\\nnent similarly uses a MLP on feature embeddings, which has\\nbeen reported to have strong generalization ability. While\\ntheir work has focused on incorporating various features\\nof users and items, we target at exploring DNNs for pure\\ncollaborative \\x0cltering systems. We show that DNNs are a\\npromising choice for modelling user{item interactions, which\\nto our knowledge has not been investigated before.\\n6. CONCLUSION AND FUTURE WORK\\nIn this work, we explored neural network architectures\\nfor collaborative \\x0cltering. We devised a general framework\\nNCF and proposed three instantiations | GMF, MLP and\\nNeuMF | that model user{item interactions in di\\x0berent\\nways. Our framework is simple and generic; it is not limited\\nto the models presented in this paper, but is designed to\\nserve as a guideline for developing deep learning methods for\\nrecommendation. This work complements the mainstream\\nshallow models for collaborative \\x0cltering, opening up a new\\navenue of research possibilities for recommendation based\\non deep learning.\\nIn future, we will study pairwise learners for NCF mod-\\nels and extend NCF to model auxiliary information, such\\nas user reviews [11], knowledge bases [45], and temporal sig-\\nnals [1]. While existing personalization models have primar-\\nily focused on individuals, it is interesting to develop models\\nfor groups of users, which help the decision-making for social\\ngroups [15, 42]. Moreover, we are particularly interested in\\nbuilding recommender systems for multi-media items, an in-\\nteresting task but has received relatively less scrutiny in the\\nrecommendation community [3]. Multi-media items, such as\\nimages and videos, contain much richer visual semantics [16,\\n41] that can re\\rect users\\' interest. To build a multi-media\\nrecommender system, we need to develop e\\x0bective methods\\nto learn from multi-view and multi-modal data [13, 40]. An-\\nother emerging direction is to explore the potential of recur-\\nrent neural networks and hashing methods [46] for providing\\ne\\x0ecient online recommendation [14, 1].\\nAcknowledgement\\nThe authors thank the anonymous reviewers for their valu-\\nable comments, which are bene\\x0ccial to the authors\\' thoughts\\non recommendation systems and the revision of the paper.\\n7. REFERENCES\\n[1] I. Bayer, X. He, B. Kanagal, and S. Rendle. A generic\\ncoordinate descent framework for learning from implicit\\nfeedback. In WWW , 2017.\\n[2] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and\\nO. Yakhnenko. Translating embeddings for modeling\\nmulti-relational data. In NIPS , pages 2787{2795, 2013.\\n[3] T. Chen, X. He, and M.-Y. Kan. Context-aware image\\ntweet modelling and recommendation. In MM, pages\\n1018{1027, 2016.[4] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra,\\nH. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir,\\net al. Wide & deep learning for recommender systems.\\narXiv preprint arXiv:1606.07792 , 2016.\\n[5] R. Collobert and J. Weston. A uni\\x0ced architecture for\\nnatural language processing: Deep neural networks with\\nmultitask learning. In ICML , pages 160{167, 2008.\\n[6] A. M. Elkahky, Y. Song, and X. He. A multi-view deep\\nlearning approach for cross domain user modeling in\\nrecommendation systems. In WWW , pages 278{288, 2015.\\n[7] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol,\\nP. Vincent, and S. Bengio. Why does unsupervised\\npre-training help deep learning? Journal of Machine\\nLearning Research , 11:625{660, 2010.\\n[8] X. Geng, H. Zhang, J. Bian, and T.-S. Chua. Learning\\nimage and user features for recommendation in social\\nnetworks. In ICCV , pages 4274{4282, 2015.\\n[9] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse recti\\x0cer\\nneural networks. In AISTATS , pages 315{323, 2011.\\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\\nlearning for image recognition. In CVPR , 2016.\\n[11] X. He, T. Chen, M.-Y. Kan, and X. Chen. TriRank:\\nReview-aware explainable recommendation by modeling\\naspects. In CIKM , pages 1661{1670, 2015.\\n[12] X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama.\\nPredicting the popularity of web 2.0 items based on user\\ncomments. In SIGIR , pages 233{242, 2014.\\n[13] X. He, M.-Y. Kan, P. Xie, and X. Chen. Comment-based\\nmulti-view clustering of web 2.0 items. In WWW , pages\\n771{782, 2014.\\n[14] X. He, H. Zhang, M.-Y. Kan, and T.-S. Chua. Fast matrix\\nfactorization for online recommendation with implicit\\nfeedback. In SIGIR , pages 549{558, 2016.\\n[15] R. Hong, Z. Hu, L. Liu, M. Wang, S. Yan, and Q. Tian.\\nUnderstanding blooming human groups in social networks.\\nIEEE Transactions on Multimedia , 17(11):1980{1988, 2015.\\n[16] R. Hong, Y. Yang, M. Wang, and X. S. Hua. Learning\\nvisual semantic relationships for e\\x0ecient visual retrieval.\\nIEEE Transactions on Big Data , 1(4):152{161, 2015.\\n[17] K. Hornik, M. Stinchcombe, and H. White. Multilayer\\nfeedforward networks are universal approximators. Neural\\nNetworks , 2(5):359{366, 1989.\\n[18] L. Hu, A. Sun, and Y. Liu. Your neighbors a\\x0bect your\\nratings: On geographical neighborhood in\\ruence to rating\\nprediction. In SIGIR , pages 345{354, 2014.\\n[19] Y. Hu, Y. Koren, and C. Volinsky. Collaborative \\x0cltering\\nfor implicit feedback datasets. In ICDM , pages 263{272,\\n2008.\\n[20] D. Kingma and J. Ba. Adam: A method for stochastic\\noptimization. In ICLR , pages 1{15, 2014.\\n[21] Y. Koren. Factorization meets the neighborhood: A\\nmultifaceted collaborative \\x0cltering model. In KDD , pages\\n426{434, 2008.\\n[22] S. Li, J. Kawale, and Y. Fu. Deep collaborative \\x0cltering via\\nmarginalized denoising auto-encoder. In CIKM , pages\\n811{820, 2015.\\n[23] D. Liang, L. Charlin, J. McInerney, and D. M. Blei.\\nModeling user exposure in recommendation. In WWW ,\\npages 951{961, 2016.\\n[24] M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich. A\\nreview of relational machine learning for knowledge graphs.\\nProceedings of the IEEE , 104:11{33, 2016.\\n[25] X. Ning and G. Karypis. Slim: Sparse linear methods for\\ntop-n recommender systems. In ICDM , pages 497{506,\\n2011.\\n[26] S. Rendle. Factorization machines. In ICDM , pages\\n995{1000, 2010.\\n[27] S. Rendle, C. Freudenthaler, Z. Gantner, and\\nL. Schmidt-Thieme. Bpr: Bayesian personalized ranking\\nfrom implicit feedback. In UAI, pages 452{461, 2009.\\n[28] S. Rendle, Z. Gantner, C. Freudenthaler, andL. Schmidt-Thieme. Fast context-aware recommendations\\nwith factorization machines. In SIGIR , pages 635{644,\\n2011.\\n[29] R. Salakhutdinov and A. Mnih. Probabilistic matrix\\nfactorization. In NIPS , pages 1{8, 2008.\\n[30] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted\\nboltzmann machines for collaborative \\x0cltering. In ICDM ,\\npages 791{798, 2007.\\n[31] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.\\nItem-based collaborative \\x0cltering recommendation\\nalgorithms. In WWW , pages 285{295, 2001.\\n[32] S. Sedhain, A. K. Menon, S. Sanner, and L. Xie. Autorec:\\nAutoencoders meet collaborative \\x0cltering. In WWW , pages\\n111{112, 2015.\\n[33] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning\\nwith neural tensor networks for knowledge base completion.\\nInNIPS , pages 926{934, 2013.\\n[34] N. Srivastava and R. R. Salakhutdinov. Multimodal\\nlearning with deep boltzmann machines. In NIPS , pages\\n2222{2230, 2012.\\n[35] F. Strub and J. Mary. Collaborative \\x0cltering with stacked\\ndenoising autoencoders and sparse inputs. In NIPS\\nWorkshop on Machine Learning for eCommerce , 2015.\\n[36] T. T. Truyen, D. Q. Phung, and S. Venkatesh. Ordinal\\nboltzmann machines for collaborative \\x0cltering. In UAI,\\npages 548{556, 2009.\\n[37] A. Van den Oord, S. Dieleman, and B. Schrauwen. Deep\\ncontent-based music recommendation. In NIPS , pages\\n2643{2651, 2013.\\n[38] H. Wang, N. Wang, and D.-Y. Yeung. Collaborative deep\\nlearning for recommender systems. In KDD , pages\\n1235{1244, 2015.\\n[39] M. Wang, W. Fu, S. Hao, D. Tao, and X. Wu. Scalable\\nsemi-supervised learning by e\\x0ecient anchor graph\\nregularization. IEEE Transactions on Knowledge and Data\\nEngineering , 28(7):1864{1877, 2016.\\n[40] M. Wang, H. Li, D. Tao, K. Lu, and X. Wu. Multimodal\\ngraph-based reranking for web image search. IEEE\\nTransactions on Image Processing , 21(11):4649{4661, 2012.\\n[41] M. Wang, X. Liu, and X. Wu. Visual classi\\x0ccation by l1\\nhypergraph modeling. IEEE Transactions on Knowledge\\nand Data Engineering , 27(9):2564{2574, 2015.\\n[42] X. Wang, L. Nie, X. Song, D. Zhang, and T.-S. Chua.\\nUnifying virtual and physical worlds: Learning towards\\nlocal and global consistency. ACM Transactions on\\nInformation Systems , 2017.\\n[43] X. Wang and Y. Wang. Improving content-based and\\nhybrid music recommendation using deep learning. In MM,\\npages 627{636, 2014.\\n[44] Y. Wu, C. DuBois, A. X. Zheng, and M. Ester.\\nCollaborative denoising auto-encoders for top-n\\nrecommender systems. In WSDM , pages 153{162, 2016.\\n[45] F. Zhang, N. J. Yuan, D. Lian, X. Xie, and W.-Y. Ma.\\nCollaborative knowledge base embedding for recommender\\nsystems. In KDD , pages 353{362, 2016.\\n[46] H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S.\\nChua. Discrete collaborative \\x0cltering. In SIGIR , pages\\n325{334, 2016.\\n[47] H. Zhang, Y. Yang, H. Luan, S. Yang, and T.-S. Chua.\\nStart from scratch: Towards automatically identifying,\\nmodeling, and naming visual attributes. In MM, pages\\n187{196, 2014.\\n[48] Y. Zheng, B. Tang, W. Ding, and H. Zhou. A neural\\nautoregressive approach to collaborative \\x0cltering. In ICML ,\\npages 764{773, 2016.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#establish connection with cassandra db\n",
        "\n",
        "cassio.init(token = ASTRA_TOKEN, database_id= VECTOR_DB_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dAp9MNmne8U",
        "outputId": "7042ef59-3c81-4a49-d05c-c9e7a2808923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 1e68fd26-681b-4b59-b823-4dd1e2204f05-us-east1.db.astra.datastax.com:29042:9a6df4e1-13db-40ef-a4fa-23ead13c781f. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 1e68fd26-681b-4b59-b823-4dd1e2204f05-us-east1.db.astra.datastax.com:29042:9a6df4e1-13db-40ef-a4fa-23ead13c781f. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(138485130409968) 1e68fd26-681b-4b59-b823-4dd1e2204f05-us-east1.db.astra.datastax.com:29042:9a6df4e1-13db-40ef-a4fa-23ead13c781f> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 1e68fd26-681b-4b59-b823-4dd1e2204f05-us-east1.db.astra.datastax.com:29042:9a6df4e1-13db-40ef-a4fa-23ead13c781f. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import langchain embeddings to embed your text\n",
        "\n",
        "OPENAI_llm = OpenAI(openai_api_key = OPENAI_TOKEN)\n",
        "embeddings = OpenAIEmbeddings(openai_api_key = OPENAI_TOKEN)"
      ],
      "metadata": {
        "id": "k97hseALn6-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "astra_db_vector_store = Cassandra(\n",
        "    embedding = embeddings,\n",
        "    table_name = \"vectore_store1\",\n",
        "    session = None,\n",
        "    keyspace = None\n",
        ")"
      ],
      "metadata": {
        "id": "OSqGle9moUkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap = 20,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "splitted_text = text_splitter.split_text(pdf_text)\n",
        "splitted_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSXcsFMEouDd",
        "outputId": "4573589a-9b74-495d-bd0b-46bc1d2eaa24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 108, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 109, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 116, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 112, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 108, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 103, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 101, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 102, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 103, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 103, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 116, which is longer than the specified 100\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 102, which is longer than the specified 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Neural Collaborative Filtering\\x03\\nXiangnan He\\nNational University of\\nSingapore, Singapore',\n",
              " 'Singapore, Singapore\\nxiangnanhe@gmail.comLizi Liao\\nNational University of\\nSingapore, Singapore',\n",
              " 'Singapore, Singapore\\nliaolizi.llz@gmail.comHanwang Zhang\\nColumbia University\\nUSA',\n",
              " 'USA\\nhanwangzhang@gmail.com\\nLiqiang Nie\\nShandong University\\nChina\\nnieliqiang@gmail.comXia Hu',\n",
              " 'Texas A&M University\\nUSA\\nhu@cse.tamu.eduTat-Seng Chua\\nNational University of\\nSingapore, Singapore',\n",
              " 'Singapore, Singapore\\ndcscts@nus.edu.sg\\nABSTRACT',\n",
              " 'ABSTRACT\\nIn recent years, deep neural networks have yielded immense',\n",
              " 'success on speech recognition, computer vision and natural',\n",
              " 'language processing. However, the exploration of deep neu-',\n",
              " 'ral networks on recommender systems has received relatively',\n",
              " 'less scrutiny. In this work, we strive to develop techniques',\n",
              " 'based on neural networks to tackle the key problem in rec-',\n",
              " 'ommendation | collaborative \\x0cltering | on the basis of\\nimplicit feedback.',\n",
              " 'implicit feedback.\\nAlthough some recent work has employed deep learning',\n",
              " 'for recommendation, they primarily used it to model auxil-',\n",
              " 'iary information, such as textual descriptions of items and',\n",
              " 'acoustic features of musics. When it comes to model the key',\n",
              " 'factor in collaborative \\x0cltering | the interaction between',\n",
              " 'user and item features, they still resorted to matrix factor-',\n",
              " 'ization and applied an inner product on the latent features\\nof users and items.',\n",
              " 'of users and items.\\nBy replacing the inner product with a neural architecture',\n",
              " 'that can learn an arbitrary function from data, we present',\n",
              " 'a general framework named NCF, short for Neural network-',\n",
              " 'based Collaborative Filtering . NCF is generic and can ex-',\n",
              " 'press and generalize matrix factorization under its frame-',\n",
              " 'work. To supercharge NCF modelling with non-linearities,',\n",
              " 'we propose to leverage a multi-layer perceptron to learn the',\n",
              " 'user{item interaction function. Extensive experiments on',\n",
              " 'two real-world datasets show signi\\x0ccant improvements of our',\n",
              " 'proposed NCF framework over the state-of-the-art methods.',\n",
              " 'Empirical evidence shows that using deeper layers of neural',\n",
              " 'networks o\\x0bers better recommendation performance.\\nKeywords',\n",
              " 'Keywords\\nCollaborative Filtering, Neural Networks, Deep Learning,',\n",
              " 'Matrix Factorization, Implicit Feedback\\n\\x03NExT research is supported by the National Research',\n",
              " \"Foundation, Prime Minister's O\\x0ece, Singapore under its\\nIRC@SG Funding Initiative.\",\n",
              " 'c\\r2017 International World Wide Web Conference Committee',\n",
              " '(IW3C2), published under Creative Commons CC BY 4.0 License.',\n",
              " 'WWW 2017, April 3â€“7, 2017, Perth, Australia.\\nACM 978-1-4503-4913-0/17/04.',\n",
              " 'http://dx.doi.org/10.1145/3038912.3052569\\n.1. INTRODUCTION',\n",
              " '.1. INTRODUCTION\\nIn the era of information explosion, recommender systems',\n",
              " 'play a pivotal role in alleviating information overload, hav-',\n",
              " 'ing been widely adopted by many online services, including',\n",
              " 'E-commerce, online news and social media sites. The key to',\n",
              " \"a personalized recommender system is in modelling users'\",\n",
              " 'preference on items based on their past interactions ( e.g.,',\n",
              " 'ratings and clicks), known as collaborative \\x0cltering [31, 46].',\n",
              " 'Among the various collaborative \\x0cltering techniques, matrix',\n",
              " 'factorization (MF) [14, 21] is the most popular one, which',\n",
              " 'projects users and items into a shared latent space, using',\n",
              " 'a vector of latent features to represent a user or an item.',\n",
              " \"Thereafter a user's interaction on an item is modelled as the\\ninner product of their latent vectors.\",\n",
              " 'Popularized by the Net\\rix Prize, MF has become the de',\n",
              " 'facto approach to latent factor model-based recommenda-',\n",
              " 'tion. Much research e\\x0bort has been devoted to enhancing',\n",
              " 'MF, such as integrating it with neighbor-based models [21],',\n",
              " 'combining it with topic models of item content [38], and ex-',\n",
              " 'tending it to factorization machines [26] for a generic mod-',\n",
              " 'elling of features. Despite the e\\x0bectiveness of MF for collab-',\n",
              " 'orative \\x0cltering, it is well-known that its performance can be',\n",
              " 'hindered by the simple choice of the interaction function |',\n",
              " 'inner product. For example, for the task of rating prediction',\n",
              " 'on explicit feedback, it is well known that the performance',\n",
              " 'of the MF model can be improved by incorporating user',\n",
              " 'and item bias terms into the interaction function1. While',\n",
              " 'it seems to be just a trivial tweak for the inner product',\n",
              " 'operator [14], it points to the positive e\\x0bect of designing a',\n",
              " 'better, dedicated interaction function for modelling the la-',\n",
              " 'tent feature interactions between users and items. The inner',\n",
              " 'product, which simply combines the multiplication of latent',\n",
              " 'features linearly, may not be su\\x0ecient to capture the com-\\nplex structure of user interaction data.',\n",
              " 'This paper explores the use of deep neural networks for',\n",
              " 'learning the interaction function from data, rather than a',\n",
              " 'handcraft that has been done by many previous work [18,',\n",
              " '21]. The neural network has been proven to be capable of',\n",
              " 'approximating any continuous function [17], and more re-',\n",
              " 'cently deep neural networks (DNNs) have been found to be',\n",
              " 'e\\x0bective in several domains, ranging from computer vision,',\n",
              " 'speech recognition, to text processing [5, 10, 15, 47]. How-',\n",
              " 'ever, there is relatively little work on employing DNNs for',\n",
              " 'recommendation in contrast to the vast amount of literature',\n",
              " '1http://alex.smola.org/teaching/berkeley2012/slides/8_',\n",
              " 'Recommender.pdfarXiv:1708.05031v2  [cs.IR]  26 Aug 2017on MF methods. Although some recent advances [37, 38,',\n",
              " '45] have applied DNNs to recommendation tasks and shown',\n",
              " 'promising results, they mostly used DNNs to model auxil-',\n",
              " 'iary information, such as textual description of items, audio',\n",
              " 'features of musics, and visual content of images. With re-',\n",
              " 'gards to modelling the key collaborative \\x0cltering e\\x0bect, they',\n",
              " 'still resorted to MF, combining user and item latent features\\nusing an inner product.',\n",
              " 'This work addresses the aforementioned research prob-',\n",
              " 'lems by formalizing a neural network modelling approach for',\n",
              " 'collaborative \\x0cltering. We focus on implicit feedback, which',\n",
              " \"indirectly re\\rects users' preference through behaviours like\",\n",
              " 'watching videos, purchasing products and clicking items.',\n",
              " 'Compared to explicit feedback ( i.e., ratings and reviews),',\n",
              " 'implicit feedback can be tracked automatically and is thus',\n",
              " 'much easier to collect for content providers. However, it is',\n",
              " 'more challenging to utilize, since user satisfaction is not ob-',\n",
              " 'served and there is a natural scarcity of negative feedback.',\n",
              " 'In this paper, we explore the central theme of how to utilize',\n",
              " 'DNNs to model noisy implicit feedback signals.\\nThe main contributions of this work are as follows.',\n",
              " '1. We present a neural network architecture to model',\n",
              " 'latent features of users and items and devise a gen-',\n",
              " 'eral framework NCF for collaborative \\x0cltering based\\non neural networks.',\n",
              " 'on neural networks.\\n2. We show that MF can be interpreted as a specialization',\n",
              " 'of NCF and utilize a multi-layer perceptron to endow',\n",
              " 'NCF modelling with a high level of non-linearities.',\n",
              " '3. We perform extensive experiments on two real-world',\n",
              " 'datasets to demonstrate the e\\x0bectiveness of our NCF',\n",
              " 'approaches and the promise of deep learning for col-\\nlaborative \\x0cltering.\\n2. PRELIMINARIES',\n",
              " '2. PRELIMINARIES\\nWe \\x0crst formalize the problem and discuss existing solu-',\n",
              " 'tions for collaborative \\x0cltering with implicit feedback. We',\n",
              " 'then shortly recapitulate the widely used MF model, high-',\n",
              " 'lighting its limitation caused by using an inner product.\\n2.1 Learning from Implicit Data',\n",
              " 'LetMandNdenote the number of users and items,',\n",
              " \"respectively. We de\\x0cne the user{item interaction matrix\\nY2RM\\x02Nfrom users' implicit feedback as,\",\n",
              " 'yui=(\\n1;if interaction (user u, itemi) is observed;\\n0;otherwise.(1)',\n",
              " '0;otherwise.(1)\\nHere a value of 1 for yuiindicates that there is an interac-',\n",
              " 'tion between user uand itemi; however, it does not mean u',\n",
              " 'actually likes i. Similarly, a value of 0 does not necessarily',\n",
              " 'meanudoes not like i, it can be that the user is not aware',\n",
              " 'of the item. This poses challenges in learning from implicit',\n",
              " \"data, since it provides only noisy signals about users' pref-\",\n",
              " \"erence. While observed entries at least re\\rect users' interest\",\n",
              " 'on items, the unobserved entries can be just missing data',\n",
              " 'and there is a natural scarcity of negative feedback.',\n",
              " 'The recommendation problem with implicit feedback is',\n",
              " 'formulated as the problem of estimating the scores of unob-',\n",
              " 'served entries in Y, which are used for ranking the items.',\n",
              " 'Model-based approaches assume that data can be generated',\n",
              " '(or described) by an underlying model. Formally, they can',\n",
              " 'be abstracted as learning ^ yui=f(u;ij\\x02);where ^yuidenotes\\nu1\\nu2\\nu3\\nu4i1i2i3i4i5\\n11101\\n01100\\n01110',\n",
              " \"11101\\n01100\\n01110\\n10111\\nitems \\nusers (a) user{item matrix\\np1\\np2\\np3p4p'4 (b) user latent space\",\n",
              " \"Figure 1: An example illustrates MF's limitation.\",\n",
              " 'From data matrix (a), u4is most similar to u1, fol-',\n",
              " 'lowed by u3, and lastly u2. However in the latent',\n",
              " 'space (b), placing p4closest to p1makes p4closer to\\np2than p3, incurring a large ranking loss.',\n",
              " 'the predicted score of interaction yui, \\x02 denotes model pa-',\n",
              " 'rameters, and fdenotes the function that maps model pa-',\n",
              " 'rameters to the predicted score (which we term as an inter-\\naction function ).',\n",
              " 'action function ).\\nTo estimate parameters \\x02, existing approaches generally',\n",
              " 'follow the machine learning paradigm that optimizes an ob-',\n",
              " 'jective function. Two types of objective functions are most',\n",
              " 'commonly used in literature | pointwise loss [14, 19] and',\n",
              " 'pairwise loss [27, 33]. As a natural extension of abundant',\n",
              " 'work on explicit feedback [21, 46], methods on pointwise',\n",
              " 'learning usually follow a regression framework by minimiz-',\n",
              " 'ing the squared loss between ^ yuiand its target value yui.',\n",
              " 'To handle the absence of negative data, they have either',\n",
              " 'treated all unobserved entries as negative feedback, or sam-',\n",
              " 'pled negative instances from unobserved entries [14]. For',\n",
              " 'pairwise learning [27, 44], the idea is that observed entries',\n",
              " 'should be ranked higher than the unobserved ones. As such,',\n",
              " 'instead of minimizing the loss between ^ yuiandyui, pairwise',\n",
              " 'learning maximizes the margin between observed entry ^ yui\\nand unobserved entry ^ yuj.',\n",
              " 'Moving one step forward, our NCF framework parame-',\n",
              " 'terizes the interaction function fusing neural networks to',\n",
              " 'estimate ^yui. As such, it naturally supports both pointwise\\nand pairwise learning.',\n",
              " '2.2 Matrix Factorization\\nMF associates each user and item with a real-valued vector',\n",
              " 'of latent features. Let puandqidenote the latent vector for',\n",
              " 'useruand itemi, respectively; MF estimates an interaction\\nyuias the inner product of puandqi:',\n",
              " '^yui=f(u;ijpu;qi) =pT\\nuqi=KX\\nk=1pukqik; (2)\\nwhereKdenotes the dimension of the latent space. As we',\n",
              " 'can see, MF models the two-way interaction of user and item',\n",
              " 'latent factors, assuming each dimension of the latent space',\n",
              " 'is independent of each other and linearly combining them',\n",
              " 'with the same weight. As such, MF can be deemed as a\\nlinear model of latent factors.',\n",
              " 'Figure 1 illustrates how the inner product function can',\n",
              " 'limit the expressiveness of MF. There are two settings to be',\n",
              " 'stated clearly beforehand to understand the example well.',\n",
              " 'First, since MF maps users and items to the same latent',\n",
              " 'space, the similarity between two users can also be measured',\n",
              " 'with an inner product, or equivalently2, the cosine of the',\n",
              " 'angle between their latent vectors. Second, without loss of',\n",
              " '2Assuming latent vectors are of a unit length.Input Layer (Sparse)Embedding LayerNeural CF LayersOutput Layer',\n",
              " '1000 00â€¦â€¦\\nUser ( u)0000 10â€¦â€¦\\nItem ( i)User Latent Vector Item Latent VectorLayer 1Layer 2Layer X',\n",
              " 'â€¦â€¦Score TargetTrainingÅ·uiyui',\n",
              " 'PMÃ—K= {puk} QNÃ—K= {qik}Figure 2: Neural collaborative \\x0cltering framework',\n",
              " 'generality, we use the Jaccard coe\\x0ecient3as the ground-',\n",
              " 'truth similarity of two users that MF needs to recover.',\n",
              " 'Let us \\x0crst focus on the \\x0crst three rows (users) in Fig-',\n",
              " 'ure 1a. It is easy to have s23(0:66)> s 12(0:5)> s 13(0:4).',\n",
              " 'As such, the geometric relations of p1;p2;andp3in the la-',\n",
              " 'tent space can be plotted as in Figure 1b. Now, let us con-',\n",
              " 'sider a new user u4, whose input is given as the dashed line',\n",
              " 'in Figure 1a. We can have s41(0:6)> s 43(0:4)> s 42(0:2),',\n",
              " 'meaning that u4is most similar to u1, followed by u3, and',\n",
              " 'lastlyu2. However, if a MF model places p4closest to p1',\n",
              " '(the two options are shown in Figure 1b with dashed lines),',\n",
              " 'it will result in p4closer to p2thanp3, which unfortunately\\nwill incur a large ranking loss.',\n",
              " 'The above example shows the possible limitation of MF',\n",
              " 'caused by the use of a simple and \\x0cxed inner product to esti-',\n",
              " 'mate complex user{item interactions in the low-dimensional',\n",
              " 'latent space. We note that one way to resolve the issue is',\n",
              " 'to use a large number of latent factors K. However, it may',\n",
              " 'adversely hurt the generalization of the model ( e.g., over-',\n",
              " 'tting the data), especially in sparse settings [26]. In this',\n",
              " 'work, we address the limitation by learning the interaction\\nfunction using DNNs from data.',\n",
              " '3. NEURAL COLLABORATIVE FILTERING\\nWe \\x0crst present the general NCF framework, elaborat-',\n",
              " 'ing how to learn NCF with a probabilistic model that em-',\n",
              " 'phasizes the binary property of implicit data. We then',\n",
              " 'show that MF can be expressed and generalized under NCF.',\n",
              " 'To explore DNNs for collaborative \\x0cltering, we then pro-',\n",
              " 'pose an instantiation of NCF, using a multi-layer perceptron',\n",
              " '(MLP) to learn the user{item interaction function. Lastly,',\n",
              " 'we present a new neural matrix factorization model, which',\n",
              " 'ensembles MF and MLP under the NCF framework; it uni-',\n",
              " 'es the strengths of linearity of MF and non-linearity of',\n",
              " 'MLP for modelling the user{item latent structures.\\n3.1 General Framework',\n",
              " 'To permit a full neural treatment of collaborative \\x0cltering,',\n",
              " 'we adopt a multi-layer representation to model a user{item',\n",
              " 'interaction yuias shown in Figure 2, where the output of one',\n",
              " 'layer serves as the input of the next one. The bottom input\\nlayer consists of two feature vectors vU',\n",
              " 'uandvI\\nithat describe\\nuseruand itemi, respectively; they can be customized to',\n",
              " 'support a wide range of modelling of users and items, such',\n",
              " '3LetRube the set of items that user uhas interacted with,',\n",
              " 'then the Jaccard similarity of users iandjis de\\x0cned as\\nsij=jRij\\\\jR jj',\n",
              " 'sij=jRij\\\\jR jj\\njRij[jR jj.as context-aware [28, 1], content-based [3], and neighbor-',\n",
              " 'based [26]. Since this work focuses on the pure collaborative',\n",
              " 'ltering setting, we use only the identity of a user and an',\n",
              " 'item as the input feature, transforming it to a binarized',\n",
              " 'sparse vector with one-hot encoding. Note that with such a',\n",
              " 'generic feature representation for inputs, our method can be',\n",
              " 'easily adjusted to address the cold-start problem by using',\n",
              " 'content features to represent users and items.',\n",
              " 'Above the input layer is the embedding layer; it is a fully',\n",
              " 'connected layer that projects the sparse representation to',\n",
              " 'a dense vector. The obtained user (item) embedding can',\n",
              " 'be seen as the latent vector for user (item) in the context',\n",
              " 'of latent factor model. The user embedding and item em-',\n",
              " 'bedding are then fed into a multi-layer neural architecture,',\n",
              " 'which we term as neural collaborative \\x0cltering layers , to map',\n",
              " 'the latent vectors to prediction scores. Each layer of the neu-',\n",
              " 'ral CF layers can be customized to discover certain latent',\n",
              " 'structures of user{item interactions. The dimension of the',\n",
              " \"last hidden layer Xdetermines the model's capability. The\",\n",
              " 'nal output layer is the predicted score ^ yui, and training',\n",
              " 'is performed by minimizing the pointwise loss between ^ yui',\n",
              " 'and its target value yui. We note that another way to train',\n",
              " 'the model is by performing pairwise learning, such as using',\n",
              " 'the Bayesian Personalized Ranking [27] and margin-based',\n",
              " 'loss [33]. As the focus of the paper is on the neural network',\n",
              " 'modelling part, we leave the extension to pairwise learning\\nof NCF as a future work.',\n",
              " \"We now formulate the NCF's predictive model as\\n^yui=f(PTvU\\nu;QTvI\\nijP;Q;\\x02f); (3)\",\n",
              " 'ijP;Q;\\x02f); (3)\\nwhere P2RM\\x02KandQ2RN\\x02K, denoting the latent fac-',\n",
              " 'tor matrix for users and items, respectively; and \\x02 fdenotes',\n",
              " 'the model parameters of the interaction function f. Since',\n",
              " 'the function fis de\\x0cned as a multi-layer neural network, it\\ncan be formulated as\\nf(PTvU\\nu;QTvI',\n",
              " 'f(PTvU\\nu;QTvI\\ni) =\\x1eout(\\x1eX(:::\\x1e 2(\\x1e1(PTvU\\nu;QTvI\\ni)):::));\\n(4)',\n",
              " 'u;QTvI\\ni)):::));\\n(4)\\nwhere\\x1eoutand\\x1exrespectively denote the mapping function',\n",
              " 'for the output layer and x-th neural collaborative \\x0cltering',\n",
              " '(CF) layer, and there are Xneural CF layers in total.\\n3.1.1 Learning NCF',\n",
              " '3.1.1 Learning NCF\\nTo learn model parameters, existing pointwise methods [14,',\n",
              " '39] largely perform a regression with squared loss:\\nLsqr=X\\n(u;i)2Y[Y\\x00wui(yui\\x00^yui)2; (5)',\n",
              " 'whereYdenotes the set of observed interactions in Y, and',\n",
              " 'Y\\x00denotes the set of negative instances, which can be all (or',\n",
              " 'sampled from) unobserved interactions; and wuiis a hyper-',\n",
              " 'parameter denoting the weight of training instance ( u;i).',\n",
              " 'While the squared loss can be explained by assuming that',\n",
              " 'observations are generated from a Gaussian distribution [29],',\n",
              " 'we point out that it may not tally well with implicit data.',\n",
              " 'This is because for implicit data, the target value yuiis',\n",
              " 'a binarized 1 or 0 denoting whether uhas interacted with',\n",
              " 'i. In what follows, we present a probabilistic approach for',\n",
              " 'learning the pointwise NCF that pays special attention to\\nthe binary property of implicit data.',\n",
              " 'Considering the one-class nature of implicit feedback, we',\n",
              " 'can view the value of yuias a label | 1 means item iis',\n",
              " 'relevant to u, and 0 otherwise. The prediction score ^ yuithen represents how likely iis relevant to u. To endow NCF',\n",
              " 'with such a probabilistic explanation, we need to constrain',\n",
              " 'the output ^ yuiin the range of [0 ;1], which can be easily',\n",
              " 'achieved by using a probabilistic function ( e.g., theLogistic',\n",
              " 'orProbit function) as the activation function for the output',\n",
              " 'layer\\x1eout. With the above settings, we then de\\x0cne the\\nlikelihood function as\\np(Y;Y\\x00jP;Q;\\x02f) =Y',\n",
              " 'p(Y;Y\\x00jP;Q;\\x02f) =Y\\n(u;i)2Y^yuiY\\n(u;j)2Y\\x00(1\\x00^yuj):(6)',\n",
              " '(u;j)2Y\\x00(1\\x00^yuj):(6)\\nTaking the negative logarithm of the likelihood, we reach\\nL=\\x00X',\n",
              " 'L=\\x00X\\n(u;i)2Ylog ^yui\\x00X\\n(u;j)2Y\\x00log(1\\x00^yuj)\\n=\\x00X\\n(u;i)2Y[Y\\x00yuilog ^yui+ (1\\x00yui) log(1\\x00^yui):(7)',\n",
              " 'This is the objective function to minimize for the NCF meth-',\n",
              " 'ods, and its optimization can be done by performing stochas-',\n",
              " 'tic gradient descent (SGD). Careful readers might have real-',\n",
              " 'ized that it is the same as the binary cross-entropy loss , also',\n",
              " 'known as log loss . By employing a probabilistic treatment',\n",
              " 'for NCF, we address recommendation with implicit feedback',\n",
              " 'as a binary classi\\x0ccation problem. As the classi\\x0ccation-',\n",
              " 'aware log loss has rarely been investigated in recommen-',\n",
              " 'dation literature, we explore it in this work and empirically',\n",
              " 'show its e\\x0bectiveness in Section 4.3. For the negative in-',\n",
              " 'stancesY\\x00, we uniformly sample them from unobserved in-',\n",
              " 'teractions in each iteration and control the sampling ratio',\n",
              " 'w.r.t. the number of observed interactions. While a non-',\n",
              " 'uniform sampling strategy ( e.g., item popularity-biased [14,',\n",
              " '12]) might further improve the performance, we leave the\\nexploration as a future work.',\n",
              " '3.2 Generalized Matrix Factorization (GMF)\\nWe now show how MF can be interpreted as a special case',\n",
              " 'of our NCF framework. As MF is the most popular model',\n",
              " 'for recommendation and has been investigated extensively',\n",
              " 'in literature, being able to recover it allows NCF to mimic',\n",
              " 'a large family of factorization models [26].',\n",
              " 'Due to the one-hot encoding of user (item) ID of the input',\n",
              " 'layer, the obtained embedding vector can be seen as the',\n",
              " 'latent vector of user (item). Let the user latent vector pu\\nbePTvU\\nuand item latent vector qibeQTvI',\n",
              " 'i. We de\\x0cne the\\nmapping function of the \\x0crst neural CF layer as\\n\\x1e1(pu;qi) =pu\\x0cqi; (8)',\n",
              " 'where\\x0cdenotes the element-wise product of vectors. We\\nthen project the vector to the output layer:',\n",
              " '^yui=aout(hT(pu\\x0cqi)); (9)\\nwhereaoutandhdenote the activation function and edge',\n",
              " 'weights of the output layer, respectively. Intuitively, if we',\n",
              " 'use an identity function for aoutand enforce hto be a uni-',\n",
              " 'form vector of 1, we can exactly recover the MF model.',\n",
              " 'Under the NCF framework, MF can be easily general-',\n",
              " 'ized and extended. For example, if we allow hto be learnt',\n",
              " 'from data without the uniform constraint, it will result in',\n",
              " 'a variant of MF that allows varying importance of latent',\n",
              " 'dimensions. And if we use a non-linear function for aout, it',\n",
              " 'will generalize MF to a non-linear setting which might be',\n",
              " 'more expressive than the linear MF model. In this work, we',\n",
              " 'implement a generalized version of MF under NCF that usesthe sigmoid function \\x1b(x) = 1=(1 +e\\x00x) asaoutand learns',\n",
              " 'hfrom data with the log loss (Section 3.1.1). We term it as',\n",
              " 'GMF, short for Generalized Matrix Factorization .\\n3.3 Multi-Layer Perceptron (MLP)',\n",
              " 'Since NCF adopts two pathways to model users and items,',\n",
              " 'it is intuitive to combine the features of two pathways by',\n",
              " 'concatenating them. This design has been widely adopted',\n",
              " 'in multimodal deep learning work [47, 34]. However, simply',\n",
              " 'a vector concatenation does not account for any interactions',\n",
              " 'between user and item latent features, which is insu\\x0ecient',\n",
              " 'for modelling the collaborative \\x0cltering e\\x0bect. To address',\n",
              " 'this issue, we propose to add hidden layers on the concate-',\n",
              " 'nated vector, using a standard MLP to learn the interaction',\n",
              " 'between user and item latent features. In this sense, we can',\n",
              " 'endow the model a large level of \\rexibility and non-linearity',\n",
              " 'to learn the interactions between puandqi, rather than the',\n",
              " 'way of GMF that uses only a \\x0cxed element-wise product',\n",
              " 'on them. More precisely, the MLP model under our NCF\\nframework is de\\x0cned as\\nz1=\\x1e1(pu;qi) =\\x14\\npu\\nqi\\x15\\n;',\n",
              " 'pu\\nqi\\x15\\n;\\n\\x1e2(z1) =a2(WT\\n2z1+b2);\\n::::::\\n\\x1eL(zL\\x001) =aL(WT\\nLzL\\x001+bL);\\n^yui=\\x1b(hT\\x1eL(zL\\x001));(10)',\n",
              " 'where Wx,bx, andaxdenote the weight matrix, bias vec-',\n",
              " \"tor, and activation function for the x-th layer's perceptron,\",\n",
              " 'respectively. For activation functions of MLP layers, one',\n",
              " 'can freely choose sigmoid, hyperbolic tangent (tanh), and',\n",
              " 'Recti\\x0cer (ReLU), among others. We would like to ana-',\n",
              " 'lyze each function: 1) The sigmoid function restricts each',\n",
              " \"neuron to be in (0,1), which may limit the model's perfor-\",\n",
              " 'mance; and it is known to su\\x0ber from saturation, where',\n",
              " 'neurons stop learning when their output is near either 0 or',\n",
              " '1. 2) Even though tanh is a better choice and has been',\n",
              " 'widely adopted [6, 44], it only alleviates the issues of sig-',\n",
              " 'moid to a certain extent, since it can be seen as a rescaled',\n",
              " 'version of sigmoid (tanh( x=2) = 2\\x1b(x)\\x001). And 3) as',\n",
              " 'such, we opt for ReLU, which is more biologically plausi-',\n",
              " 'ble and proven to be non-saturated [9]; moreover, it encour-',\n",
              " 'ages sparse activations, being well-suited for sparse data and',\n",
              " 'making the model less likely to be over\\x0ctting. Our empirical',\n",
              " 'results show that ReLU yields slightly better performance',\n",
              " 'than tanh, which in turn is signi\\x0ccantly better than sigmoid.',\n",
              " 'As for the design of network structure, a common solution',\n",
              " 'is to follow a tower pattern, where the bottom layer is the',\n",
              " 'widest and each successive layer has a smaller number of',\n",
              " 'neurons (as in Figure 2). The premise is that by using a',\n",
              " 'small number of hidden units for higher layers, they can',\n",
              " 'learn more abstractive features of data [10]. We empirically',\n",
              " 'implement the tower structure, halving the layer size for\\neach successive higher layer.',\n",
              " '3.4 Fusion of GMF and MLP\\nSo far we have developed two instantiations of NCF |',\n",
              " 'GMF that applies a linear kernel to model the latent feature',\n",
              " 'interactions, and MLP that uses a non-linear kernel to learn',\n",
              " 'the interaction function from data. The question then arises:',\n",
              " 'how can we fuse GMF and MLP under the NCF framework,1000 00â€¦â€¦\\nUser ( u)0000 10â€¦â€¦',\n",
              " 'User ( u)0000 10â€¦â€¦\\nItem ( i)MF User Vector MF Item VectorGMF Layer â€¦â€¦Score TargetTrainingÅ·uiyui',\n",
              " 'MLP Layer 1\\nMLP User Vector MLP Item VectorElement -wise \\nProduct',\n",
              " 'Product\\nConcatenationMLP Layer 2MLP Layer XNeuMF LayerLog loss\\nğˆ',\n",
              " 'ğˆ\\nReLUReLUConcatenationFigure 3: Neural matrix factorization model',\n",
              " 'so that they can mutually reinforce each other to better\\nmodel the complex user-iterm interactions?',\n",
              " 'A straightforward solution is to let GMF and MLP share',\n",
              " 'the same embedding layer, and then combine the outputs of',\n",
              " 'their interaction functions. This way shares a similar spirit',\n",
              " 'with the well-known Neural Tensor Network (NTN) [33].',\n",
              " 'Speci\\x0ccally, the model for combining GMF with a one-layer\\nMLP can be formulated as',\n",
              " '^yui=\\x1b(hTa(pu\\x0cqi+W\\x14\\npu\\nqi\\x15\\n+b)): (11)\\nHowever, sharing embeddings of GMF and MLP might',\n",
              " 'limit the performance of the fused model. For example,',\n",
              " 'it implies that GMF and MLP must use the same size of',\n",
              " 'embeddings; for datasets where the optimal embedding size',\n",
              " 'of the two models varies a lot, this solution may fail to obtain\\nthe optimal ensemble.',\n",
              " 'To provide more \\rexibility to the fused model, we allow',\n",
              " 'GMF and MLP to learn separate embeddings, and combine',\n",
              " 'the two models by concatenating their last hidden layer.',\n",
              " 'Figure 3 illustrates our proposal, the formulation of which\\nis given as follows\\n\\x1eGMF=pG\\nu\\x0cqG\\ni;',\n",
              " 'GMF=pG\\nu\\x0cqG\\ni;\\n\\x1eMLP=aL(WT\\nL(aL\\x001(:::a2(WT\\n2\\x14\\npM\\nu\\nqM\\ni\\x15\\n+b2):::)) +bL);\\n^yui=\\x1b(hT\\x14\\n\\x1eGMF\\n\\x1eMLP\\x15\\n);',\n",
              " 'GMF\\n\\x1eMLP\\x15\\n);\\n(12)\\nwhere pG\\nuandpM\\nudenote the user embedding for GMF',\n",
              " 'and MLP parts, respectively; and similar notations of qG\\ni\\nandqM',\n",
              " 'i\\nandqM\\nifor item embeddings. As discussed before, we use',\n",
              " 'ReLU as the activation function of MLP layers. This model',\n",
              " 'combines the linearity of MF and non-linearity of DNNs for',\n",
              " 'modelling user{item latent structures. We dub this model',\n",
              " '\\\\NeuMF\", short for Neural Matrix Factorization . The deriva-',\n",
              " 'tive of the model w.r.t. each model parameter can be cal-',\n",
              " 'culated with standard back-propagation, which is omitted\\nhere due to space limitation.',\n",
              " '3.4.1 Pre-training\\nDue to the non-convexity of the objective function of NeuMF,',\n",
              " 'gradient-based optimization methods only \\x0cnd locally-optimal',\n",
              " 'solutions. It is reported that the initialization plays an im-',\n",
              " 'portant role for the convergence and performance of deep',\n",
              " 'learning models [7]. Since NeuMF is an ensemble of GMFand MLP, we propose to initialize NeuMF using the pre-',\n",
              " 'trained models of GMF and MLP.\\nWe \\x0crst train GMF and MLP with random initializations',\n",
              " 'until convergence. We then use their model parameters as',\n",
              " \"the initialization for the corresponding parts of NeuMF's\",\n",
              " 'parameters. The only tweak is on the output layer, where',\n",
              " 'we concatenate weights of the two models with\\nh \\x14\\n\\x0bhGMF\\n(1\\x00\\x0b)hMLP\\x15\\n; (13)',\n",
              " '(1\\x00\\x0b)hMLP\\x15\\n; (13)\\nwhere hGMFandhMLPdenote the hvector of the pre-',\n",
              " 'trained GMF and MLP model, respectively; and \\x0bis a',\n",
              " 'hyper-parameter determining the trade-o\\x0b between the two\\npre-trained models.',\n",
              " 'pre-trained models.\\nFor training GMF and MLP from scratch, we adopt the',\n",
              " 'Adaptive Moment Estimation (Adam) [20], which adapts',\n",
              " 'the learning rate for each parameter by performing smaller',\n",
              " 'updates for frequent and larger updates for infrequent pa-',\n",
              " 'rameters. The Adam method yields faster convergence for',\n",
              " 'both models than the vanilla SGD and relieves the pain of',\n",
              " 'tuning the learning rate. After feeding pre-trained parame-',\n",
              " 'ters into NeuMF, we optimize it with the vanilla SGD, rather',\n",
              " 'than Adam. This is because Adam needs to save momentum',\n",
              " 'information for updating parameters properly. As we ini-',\n",
              " 'tialize NeuMF with pre-trained model parameters only and',\n",
              " 'forgo saving the momentum information, it is unsuitable to',\n",
              " 'further optimize NeuMF with momentum-based methods.\\n4. EXPERIMENTS',\n",
              " '4. EXPERIMENTS\\nIn this section, we conduct experiments with the aim of',\n",
              " 'answering the following research questions:\\nRQ1 Do our proposed NCF methods outperform the state-',\n",
              " 'of-the-art implicit collaborative \\x0cltering methods?',\n",
              " 'RQ2 How does our proposed optimization framework (log',\n",
              " 'loss with negative sampling) work for the recommen-\\ndation task?',\n",
              " 'dation task?\\nRQ3 Are deeper layers of hidden units helpful for learning',\n",
              " 'from user{item interaction data?\\nIn what follows, we \\x0crst present the experimental settings,',\n",
              " 'followed by answering the above three research questions.\\n4.1 Experimental Settings',\n",
              " 'Datasets. We experimented with two publicly accessible',\n",
              " 'datasets: MovieLens4and Pinterest5. The characteristics of',\n",
              " 'the two datasets are summarized in Table 1.\\n1. MovieLens . This movie rating dataset has been',\n",
              " 'widely used to evaluate collaborative \\x0cltering algorithms.',\n",
              " 'We used the version containing one million ratings, where',\n",
              " 'each user has at least 20 ratings. While it is an explicit',\n",
              " 'feedback data, we have intentionally chosen it to investigate',\n",
              " 'the performance of learning from the implicit signal [21] of',\n",
              " 'explicit feedback. To this end, we transformed it into im-',\n",
              " 'plicit data, where each entry is marked as 0 or 1 indicating\\nwhether the user has rated the item.',\n",
              " '2. Pinterest . This implicit feedback data is constructed',\n",
              " 'by [8] for evaluating content-based image recommendation.',\n",
              " '4http://grouplens.org/datasets/movielens/1m/\\n5https://sites.google.com/site/xueatalphabeta/',\n",
              " 'academic-projectsTable 1: Statistics of the evaluation datasets.',\n",
              " 'Dataset Interaction# Item# User# Sparsity\\nMovieLens 1,000,209 3,706 6,040 95.53%',\n",
              " 'Pinterest 1,500,809 9,916 55,187 99.73%\\nThe original data is very large but highly sparse. For exam-',\n",
              " 'ple, over 20% of users have only one pin, making it di\\x0ecult',\n",
              " 'to evaluate collaborative \\x0cltering algorithms. As such, we',\n",
              " 'ltered the dataset in the same way as the MovieLens data',\n",
              " 'that retained only users with at least 20 interactions (pins).',\n",
              " 'This results in a subset of the data that contains 55 ;187',\n",
              " 'users and 1 ;500;809 interactions. Each interaction denotes',\n",
              " 'whether the user has pinned the image to her own board.',\n",
              " 'Evaluation Protocols. To evaluate the performance of',\n",
              " 'item recommendation, we adopted the leave-one-out evalu-',\n",
              " 'ation, which has been widely used in literature [1, 14, 27].',\n",
              " 'For each user, we held-out her latest interaction as the test',\n",
              " 'set and utilized the remaining data for training. Since it is',\n",
              " 'too time-consuming to rank all items for every user during',\n",
              " 'evaluation, we followed the common strategy [6, 21] that',\n",
              " 'randomly samples 100 items that are not interacted by the',\n",
              " 'user, ranking the test item among the 100 items. The perfor-',\n",
              " 'mance of a ranked list is judged by Hit Ratio (HR) and Nor-',\n",
              " 'malized Discounted Cumulative Gain (NDCG) [11]. With-',\n",
              " 'out special mention, we truncated the ranked list at 10 for',\n",
              " 'both metrics. As such, the HR intuitively measures whether',\n",
              " 'the test item is present on the top-10 list, and the NDCG',\n",
              " 'accounts for the position of the hit by assigning higher scores',\n",
              " 'to hits at top ranks. We calculated both metrics for each\\ntest user and reported the average score.',\n",
              " 'Baselines. We compared our proposed NCF methods (GMF,\\nMLP and NeuMF) with the following methods:',\n",
              " '-ItemPop . Items are ranked by their popularity judged',\n",
              " 'by the number of interactions. This is a non-personalized',\n",
              " 'method to benchmark the recommendation performance [27].',\n",
              " '-ItemKNN [31]. This is the standard item-based col-',\n",
              " 'laborative \\x0cltering method. We followed the setting of [19]\\nto adapt it for implicit data.',\n",
              " '-BPR [27]. This method optimizes the MF model of',\n",
              " 'Equation 2 with a pairwise ranking loss, which is tailored',\n",
              " 'to learn from implicit feedback. It is a highly competitive',\n",
              " 'baseline for item recommendation. We used a \\x0cxed learning',\n",
              " 'rate, varying it and reporting the best performance.',\n",
              " '-eALS [14]. This is a state-of-the-art MF method for',\n",
              " 'item recommendation. It optimizes the squared loss of Equa-',\n",
              " 'tion 5, treating all unobserved interactions as negative in-',\n",
              " 'stances and weighting them non-uniformly by the item pop-',\n",
              " 'ularity. Since eALS shows superior performance over the',\n",
              " \"uniform-weighting method WMF [19], we do not further re-\\nport WMF's performance.\",\n",
              " 'As our proposed methods aim to model the relationship',\n",
              " 'between users and items, we mainly compare with user{',\n",
              " 'item models. We leave out the comparison with item{item',\n",
              " 'models, such as SLIM [25] and CDAE [44], because the per-',\n",
              " 'formance di\\x0berence may be caused by the user models for',\n",
              " 'personalization (as they are item{item model).\\nParameter Settings. We implemented our proposed meth-',\n",
              " 'ods based on Keras6. To determine hyper-parameters of',\n",
              " 'NCF methods, we randomly sampled one interaction for\\n6https://github.com/hexiangnan/neural_',\n",
              " 'collaborative_filteringeach user as the validation data and tuned hyper-parameters',\n",
              " 'on it. All NCF models are learnt by optimizing the log loss',\n",
              " 'of Equation 7, where we sampled four negative instances',\n",
              " 'per positive instance. For NCF models that are trained',\n",
              " 'from scratch, we randomly initialized model parameters with',\n",
              " 'a Gaussian distribution (with a mean of 0 and standard',\n",
              " 'deviation of 0 :01), optimizing the model with mini-batch',\n",
              " 'Adam [20]. We tested the batch size of [128 ;256;512;1024],',\n",
              " 'and the learning rate of [0.0001 ,0.0005, 0.001, 0.005]. Since',\n",
              " 'the last hidden layer of NCF determines the model capa-',\n",
              " 'bility, we term it as predictive factors and evaluated the',\n",
              " 'factors of [8 ;16;32;64]. It is worth noting that large factors',\n",
              " 'may cause over\\x0ctting and degrade the performance. With-',\n",
              " 'out special mention, we employed three hidden layers for',\n",
              " 'MLP; for example, if the size of predictive factors is 8, then',\n",
              " 'the architecture of the neural CF layers is 32 !16!8, and',\n",
              " 'the embedding size is 16. For the NeuMF with pre-training,',\n",
              " 'was set to 0.5, allowing the pre-trained GMF and MLP to',\n",
              " \"contribute equally to NeuMF's initialization.\\n4.2 Performance Comparison (RQ1)\",\n",
              " 'Figure 4 shows the performance of HR@10 and NDCG@10',\n",
              " 'with respect to the number of predictive factors. For MF',\n",
              " 'methods BPR and eALS, the number of predictive factors',\n",
              " 'is equal to the number of latent factors. For ItemKNN, we',\n",
              " 'tested di\\x0berent neighbor sizes and reported the best per-',\n",
              " 'formance. Due to the weak performance of ItemPop, it is',\n",
              " 'omitted in Figure 4 to better highlight the performance dif-\\nference of personalized methods.',\n",
              " 'First, we can see that NeuMF achieves the best perfor-',\n",
              " 'mance on both datasets, signi\\x0ccantly outperforming the state-',\n",
              " 'of-the-art methods eALS and BPR by a large margin (on',\n",
              " 'average, the relative improvement over eALS and BPR is',\n",
              " '4:5% and 4:9%, respectively). For Pinterest, even with a',\n",
              " 'small predictive factor of 8, NeuMF substantially outper-',\n",
              " 'forms that of eALS and BPR with a large factor of 64. This',\n",
              " 'indicates the high expressiveness of NeuMF by fusing the',\n",
              " 'linear MF and non-linear MLP models. Second, the other',\n",
              " 'two NCF methods | GMF and MLP | also show quite',\n",
              " 'strong performance. Between them, MLP slightly under-',\n",
              " 'performs GMF. Note that MLP can be further improved by',\n",
              " 'adding more hidden layers (see Section 4.4), and here we',\n",
              " 'only show the performance of three layers. For small pre-',\n",
              " 'dictive factors, GMF outperforms eALS on both datasets;',\n",
              " 'although GMF su\\x0bers from over\\x0ctting for large factors, its',\n",
              " 'best performance obtained is better than (or on par with)',\n",
              " 'that of eALS. Lastly, GMF shows consistent improvements',\n",
              " 'over BPR, admitting the e\\x0bectiveness of the classi\\x0ccation-',\n",
              " 'aware log loss for the recommendation task, since GMF and',\n",
              " 'BPR learn the same MF model but with di\\x0berent objective\\nfunctions.',\n",
              " 'functions.\\nFigure 5 shows the performance of Top- Krecommended',\n",
              " 'lists where the ranking position Kranges from 1 to 10. To',\n",
              " 'make the \\x0cgure more clear, we show the performance of',\n",
              " 'NeuMF rather than all three NCF methods. As can be',\n",
              " 'seen, NeuMF demonstrates consistent improvements over',\n",
              " 'other methods across positions, and we further conducted',\n",
              " 'one-sample paired t-tests, verifying that all improvements',\n",
              " 'are statistically signi\\x0ccant for p<0:01. For baseline meth-',\n",
              " 'ods, eALS outperforms BPR on MovieLens with about 5 :1%',\n",
              " 'relative improvement, while underperforms BPR on Pinter-',\n",
              " \"est in terms of NDCG. This is consistent with [14]'s \\x0cnding\",\n",
              " 'that BPR can be a strong performer for ranking performance0.55 0.6 0.65 0.7 0.75 \\n8 16 32 64 HR@10',\n",
              " '8 16 32 64 HR@10 \\nFactors MovieLens \\nItemKNN BPR \\neALS GMF \\nMLP NeuMF (a) MovieLens | HR@10',\n",
              " '0.3 0.34 0.38 0.42 0.46 \\n8 16 32 64 NDCG@10 \\nFactors MovieLens \\nItemKNN BPR \\neALS GMF',\n",
              " 'eALS GMF \\nMLP NeuMF (b) MovieLens | NDCG@10\\n0.78 0.81 0.84 0.87 0.9 \\n8 16 32 64 HR@10',\n",
              " '8 16 32 64 HR@10 \\nFactors Pinterest \\nItemKNN BPR \\neALS GMF \\nMLP NeuMF (c) Pinterest | HR@10',\n",
              " '0.48 0.5 0.52 0.54 0.56 \\n8 16 32 64 NDCG@10 \\nFactors Pinterest \\nItemKNN BPR \\neALS GMF',\n",
              " 'eALS GMF \\nMLP NeuMF (d) Pinterest | NDCG@10',\n",
              " 'Figure 4: Performance of HR@10 and NDCG@10 w.r.t. the number of predictive factors on the two datasets.',\n",
              " '0.1 0.25 0.4 0.55 0.7 \\n1 2 3 4 5 6 7 8 9 10 HR@K \\nKMovieLens \\nItemPop ItemKNN \\nBPR eALS \\nNeuMF',\n",
              " 'BPR eALS \\nNeuMF \\n(a) MovieLens | HR@K\\n0.1 0.18 0.26 0.34 0.42 \\n1 2 3 4 5 6 7 8 9 10 NDCG@K',\n",
              " 'KMovieLens \\nItemPop ItemKNN \\nBPR eALS \\nNeuMF (b) MovieLens | NDCG@K\\n0.1 0.3 0.5 0.7 0.9',\n",
              " '0.1 0.3 0.5 0.7 0.9 \\n1 2 3 4 5 6 7 8 9 10 HR@K \\nKPinterest \\nItemPop \\nItemKNN \\nBPR \\neALS',\n",
              " 'ItemKNN \\nBPR \\neALS \\nNeuMF (c) Pinterest | HR@K\\n0.1 0.22 0.34 0.46 0.58 \\n1 2 3 4 5 6 7 8 9 10 NDCG@K',\n",
              " 'KPinterest \\nItemPop \\nItemKNN \\nBPR \\neALS \\nNeuMF (d) Pinterest | NDCG@K',\n",
              " 'Figure 5: Evaluation of Top- Kitem recommendation where Kranges from 1to10on the two datasets.',\n",
              " 'owing to its pairwise ranking-aware learner. The neighbor-',\n",
              " 'based ItemKNN underperforms model-based methods. And',\n",
              " 'ItemPop performs the worst, indicating the necessity of mod-',\n",
              " \"eling users' personalized preferences, rather than just recom-\\nmending popular items to users.\",\n",
              " '4.2.1 Utility of Pre-training\\nTo demonstrate the utility of pre-training for NeuMF, we',\n",
              " 'compared the performance of two versions of NeuMF |',\n",
              " 'with and without pre-training. For NeuMF without pre-',\n",
              " 'training, we used the Adam to learn it with random ini-',\n",
              " 'tializations. As shown in Table 2, the NeuMF with pre-',\n",
              " 'training achieves better performance in most cases; only',\n",
              " 'for MovieLens with a small predictive factors of 8, the pre-',\n",
              " 'training method performs slightly worse. The relative im-',\n",
              " 'provements of the NeuMF with pre-training are 2 :2% and',\n",
              " '1:1% for MovieLens and Pinterest, respectively. This re-',\n",
              " 'sult justi\\x0ces the usefulness of our pre-training method for\\ninitializing NeuMF.',\n",
              " 'initializing NeuMF.\\nTable 2: Performance of NeuMF with and without\\npre-training.',\n",
              " 'pre-training.\\nWith Pre-training Without Pre-training\\nFactors HR@10 NDCG@10 HR@10 NDCG@10\\nMovieLens',\n",
              " 'MovieLens\\n8 0.684 0.403 0.688 0.410\\n16 0.707 0.426 0.696 0.420\\n32 0.726 0.445 0.701 0.425',\n",
              " '64 0.730 0.447 0.705 0.426\\nPinterest\\n8 0.878 0.555 0.869 0.546\\n16 0.880 0.558 0.871 0.547',\n",
              " '32 0.879 0.555 0.870 0.549\\n64 0.877 0.552 0.872 0.5514.3 Log Loss with Negative Sampling (RQ2)',\n",
              " 'To deal with the one-class nature of implicit feedback,',\n",
              " 'we cast recommendation as a binary classi\\x0ccation task. By',\n",
              " 'viewing NCF as a probabilistic model, we optimized it with',\n",
              " 'the log loss. Figure 6 shows the training loss (averaged',\n",
              " 'over all instances) and recommendation performance of NCF',\n",
              " 'methods of each iteration on MovieLens. Results on Pinter-',\n",
              " 'est show the same trend and thus they are omitted due to',\n",
              " 'space limitation. First, we can see that with more iterations,',\n",
              " 'the training loss of NCF models gradually decreases and',\n",
              " 'the recommendation performance is improved. The most',\n",
              " 'e\\x0bective updates are occurred in the \\x0crst 10 iterations, and',\n",
              " 'more iterations may over\\x0ct a model ( e.g., although the train-',\n",
              " 'ing loss of NeuMF keeps decreasing after 10 iterations, its',\n",
              " 'recommendation performance actually degrades). Second,',\n",
              " 'among the three NCF methods, NeuMF achieves the lowest',\n",
              " 'training loss, followed by MLP, and then GMF. The rec-',\n",
              " 'ommendation performance also shows the same trend that',\n",
              " 'NeuMF>MLP>GMF. The above \\x0cndings provide empir-',\n",
              " 'ical evidence for the rationality and e\\x0bectiveness of optimiz-',\n",
              " 'ing the log loss for learning from implicit data.',\n",
              " 'An advantage of pointwise log loss over pairwise objective',\n",
              " 'functions [27, 33] is the \\rexible sampling ratio for negative',\n",
              " 'instances. While pairwise objective functions can pair only',\n",
              " 'one sampled negative instance with a positive instance, we',\n",
              " 'can \\rexibly control the sampling ratio of a pointwise loss. To',\n",
              " 'illustrate the impact of negative sampling for NCF methods,',\n",
              " 'we show the performance of NCF methods w.r.t. di\\x0berent',\n",
              " 'negative sampling ratios in Figure 7. It can be clearly seen',\n",
              " 'that just one negative sample per positive instance is insuf-',\n",
              " 'cient to achieve optimal performance, and sampling more',\n",
              " 'negative instances is bene\\x0ccial. Comparing GMF to BPR,',\n",
              " 'we can see the performance of GMF with a sampling ratio',\n",
              " 'of one is on par with BPR, while GMF signi\\x0ccantly betters0.1 0.2 0.3 0.4 0.5',\n",
              " '0 10 20 30 40 50 Training Loss \\nIteration MovieLens \\nGMF \\nMLP \\nNeuMF (a) Training Loss',\n",
              " '0.1 0.3 0.5 0.7 \\n0 10 20 30 40 50 HR@10 \\nIteration MovieLens \\nGMF \\nMLP \\nNeuMF (b) HR@10',\n",
              " 'MLP \\nNeuMF (b) HR@10\\n00.1 0.2 0.3 0.4 0.5 \\n0 10 20 30 40 50 NDCG@10 \\nIteration MovieLens \\nGMF \\nMLP',\n",
              " 'GMF \\nMLP \\nNeuMF (c) NDCG@10',\n",
              " 'Figure 6: Training loss and recommendation performance of NCF methods w.r.t. the number of iterations',\n",
              " 'on MovieLens (factors=8).\\n0.62 0.64 0.66 0.68 0.7 0.72 \\n1 2 3 4 5 6 7 8 9 10 HR@10',\n",
              " 'Number of Negatives MovieLens \\nNeuMF \\nGMF \\nMLP \\nBPR \\n(a) MovieLens | HR@10\\n0.36 0.38 0.4 0.42 0.44',\n",
              " '1 2 3 4 5 6 7 8 9 10 NDCG@10 \\nNumber of Negatives MovieLens \\nNeuMF \\nGMF \\nMLP',\n",
              " 'NeuMF \\nGMF \\nMLP \\nBPR (b) MovieLens | NDCG@10\\n0.84 0.85 0.86 0.87 0.88 0.89',\n",
              " '1 2 3 4 5 6 7 8 9 10 HR@10 \\nNumber of Negatives Pinterest \\nNeuMF \\nGMF \\nMLP',\n",
              " 'NeuMF \\nGMF \\nMLP \\nBPR (c) Pinterest | HR@10\\n0.52 0.53 0.54 0.55 0.56 0.57',\n",
              " '1 2 3 4 5 6 7 8 9 10 NDCG@10 \\nNumber of Negatives Pinterest \\nNeuMF GMF',\n",
              " 'NeuMF GMF \\nMLP BPR (d) Pinterest | NDCG@10',\n",
              " 'Figure 7: Performance of NCF methods w.r.t. the number of negative samples per positive instance (fac-',\n",
              " 'tors=16). The performance of BPR is also shown, which samples only one negative instance to pair with a',\n",
              " 'positive instance for learning.\\nBPR with larger sampling ratios. This shows the advan-',\n",
              " 'tage of pointwise log loss over the pairwise BPR loss. For',\n",
              " 'both datasets, the optimal sampling ratio is around 3 to 6.',\n",
              " 'On Pinterest, we \\x0cnd that when the sampling ratio is larger',\n",
              " 'than 7, the performance of NCF methods starts to drop. It',\n",
              " 'reveals that setting the sampling ratio too aggressively may\\nadversely hurt the performance.',\n",
              " '4.4 Is Deep Learning Helpful? (RQ3)\\nAs there is little work on learning user{item interaction',\n",
              " 'function with neural networks, it is curious to see whether',\n",
              " 'using a deep network structure is bene\\x0ccial to the recom-',\n",
              " 'mendation task. Towards this end, we further investigated',\n",
              " 'MLP with di\\x0berent number of hidden layers. The results',\n",
              " 'are summarized in Table 3 and 4. The MLP-3 indicates',\n",
              " 'the MLP method with three hidden layers (besides the em-',\n",
              " 'bedding layer), and similar notations for others. As we can',\n",
              " 'see, even for models with the same capability, stacking more',\n",
              " 'layers are bene\\x0ccial to performance. This result is highly',\n",
              " 'encouraging, indicating the e\\x0bectiveness of using deep mod-',\n",
              " 'els for collaborative recommendation. We attribute the im-',\n",
              " 'provement to the high non-linearities brought by stacking',\n",
              " 'more non-linear layers. To verify this, we further tried stack-',\n",
              " 'ing linear layers, using an identity function as the activation',\n",
              " 'function. The performance is much worse than using the\\nReLU unit.',\n",
              " 'ReLU unit.\\nFor MLP-0 that has no hidden layers ( i.e.,the embedding',\n",
              " 'layer is directly projected to predictions), the performance is',\n",
              " 'very weak and is not better than the non-personalized Item-',\n",
              " 'Pop. This veri\\x0ces our argument in Section 3.3 that simply',\n",
              " 'concatenating user and item latent vectors is insu\\x0ecient for',\n",
              " 'modelling their feature interactions, and thus the necessity',\n",
              " 'of transforming it with hidden layers.Table 3: HR@10 of MLP with di\\x0berent layers.',\n",
              " 'Factors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4\\nMovieLens\\n8 0.452 0.628 0.655 0.671 0.678',\n",
              " '16 0.454 0.663 0.674 0.684 0.690\\n32 0.453 0.682 0.687 0.692 0.699\\n64 0.453 0.687 0.696 0.702 0.707',\n",
              " 'Pinterest\\n8 0.275 0.848 0.855 0.859 0.862\\n16 0.274 0.855 0.861 0.865 0.867',\n",
              " '32 0.273 0.861 0.863 0.868 0.867\\n64 0.274 0.864 0.867 0.869 0.873\\n5. RELATED WORK',\n",
              " '5. RELATED WORK\\nWhile early literature on recommendation has largely fo-',\n",
              " 'cused on explicit feedback [30, 31], recent attention is in-',\n",
              " 'creasingly shifting towards implicit data [1, 14, 23]. The',\n",
              " 'collaborative \\x0cltering (CF) task with implicit feedback is',\n",
              " 'usually formulated as an item recommendation problem, for',\n",
              " 'which the aim is to recommend a short list of items to users.',\n",
              " 'In contrast to rating prediction that has been widely solved',\n",
              " 'by work on explicit feedback, addressing the item recommen-',\n",
              " 'dation problem is more practical but challenging [1, 11]. One',\n",
              " 'key insight is to model the missing data, which are always',\n",
              " 'ignored by the work on explicit feedback [21, 48]. To tailor',\n",
              " 'latent factor models for item recommendation with implicit',\n",
              " 'feedback, early work [19, 27] applies a uniform weighting',\n",
              " 'where two strategies have been proposed | which either',\n",
              " 'treated all missing data as negative instances [19] or sam-',\n",
              " 'pled negative instances from missing data [27]. Recently, He',\n",
              " 'et al. [14] and Liang et al. [23] proposed dedicated models',\n",
              " 'to weight missing data, and Rendle et al. [1] developed anTable 4: NDCG@10 of MLP with di\\x0berent layers.',\n",
              " 'Factors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4\\nMovieLens\\n8 0.253 0.359 0.383 0.399 0.406',\n",
              " '16 0.252 0.391 0.402 0.410 0.415\\n32 0.252 0.406 0.410 0.425 0.423\\n64 0.251 0.409 0.417 0.426 0.432',\n",
              " 'Pinterest\\n8 0.141 0.526 0.534 0.536 0.539\\n16 0.141 0.532 0.536 0.538 0.544',\n",
              " '32 0.142 0.537 0.538 0.542 0.546\\n64 0.141 0.538 0.542 0.545 0.550',\n",
              " 'implicit coordinate descent (iCD) solution for feature-based',\n",
              " 'factorization models, achieving state-of-the-art performance',\n",
              " 'for item recommendation. In the following, we discuss rec-',\n",
              " 'ommendation works that use neural networks.\\nThe early pioneer work by Salakhutdinov et al. [30] pro-',\n",
              " 'posed a two-layer Restricted Boltzmann Machines (RBMs)',\n",
              " \"to model users' explicit ratings on items. The work was been\",\n",
              " 'later extended to model the ordinal nature of ratings [36].',\n",
              " 'Recently, autoencoders have become a popular choice for',\n",
              " 'building recommendation systems [32, 22, 35]. The idea of',\n",
              " 'user-based AutoRec [32] is to learn hidden structures that',\n",
              " \"can reconstruct a user's ratings given her historical ratings\",\n",
              " 'as inputs. In terms of user personalization, this approach',\n",
              " 'shares a similar spirit as the item{item model [31, 25] that',\n",
              " 'represents a user as her rated items. To avoid autoencoders',\n",
              " 'learning an identity function and failing to generalize to un-',\n",
              " 'seen data, denoising autoencoders (DAEs) have been applied',\n",
              " 'to learn from intentionally corrupted inputs [22, 35]. More',\n",
              " 'recently, Zheng et al. [48] presented a neural autoregressive',\n",
              " 'method for CF. While the previous e\\x0bort has lent support',\n",
              " 'to the e\\x0bectiveness of neural networks for addressing CF,',\n",
              " 'most of them focused on explicit ratings and modelled the',\n",
              " 'observed data only. As a result, they can easily fail to learn',\n",
              " \"users' preference from the positive-only implicit data.\",\n",
              " 'Although some recent works [6, 37, 38, 43, 45] have ex-',\n",
              " 'plored deep learning models for recommendation based on',\n",
              " 'implicit feedback, they primarily used DNNs for modelling',\n",
              " 'auxiliary information, such as textual description of items [38],',\n",
              " 'acoustic features of musics [37, 43], cross-domain behaviors',\n",
              " 'of users [6], and the rich information in knowledge bases [45].',\n",
              " 'The features learnt by DNNs are then integrated with MF',\n",
              " 'for CF. The work that is most relevant to our work is [44],',\n",
              " 'which presents a collaborative denoising autoencoder (CDAE)',\n",
              " 'for CF with implicit feedback. In contrast to the DAE-based',\n",
              " 'CF [35], CDAE additionally plugs a user node to the input',\n",
              " \"of autoencoders for reconstructing the user's ratings. As\",\n",
              " 'shown by the authors, CDAE is equivalent to the SVD++',\n",
              " 'model [21] when the identity function is applied to acti-',\n",
              " 'vate the hidden layers of CDAE. This implies that although',\n",
              " 'CDAE is a neural modelling approach for CF, it still applies',\n",
              " 'a linear kernel ( i.e.,inner product) to model user{item inter-',\n",
              " 'actions. This may partially explain why using deep layers for',\n",
              " 'CDAE does not improve the performance ( cf.Section 6 of',\n",
              " '[44]). Distinct from CDAE, our NCF adopts a two-pathway',\n",
              " 'architecture, modelling user{item interactions with a multi-',\n",
              " 'layer feedforward neural network. This allows NCF to learn',\n",
              " 'an arbitrary function from the data, being more powerful',\n",
              " 'and expressive than the \\x0cxed inner product function.',\n",
              " 'Along a similar line, learning the relations of two enti-',\n",
              " 'ties has been intensively studied in literature of knowledgegraphs [2, 33]. Many relational machine learning methods',\n",
              " 'have been devised [24]. The one that is most similar to our',\n",
              " 'proposal is the Neural Tensor Network (NTN) [33], which',\n",
              " 'uses neural networks to learn the interaction of two entities',\n",
              " 'and shows strong performance. Here we focus on a di\\x0ber-',\n",
              " 'ent problem setting of CF. While the idea of NeuMF that',\n",
              " 'combines MF with MLP is partially inspired by NTN, our',\n",
              " 'NeuMF is more \\rexible and generic than NTN, in terms of',\n",
              " 'allowing MF and MLP learning di\\x0berent sets of embeddings.',\n",
              " 'More recently, Google publicized their Wide & Deep learn-',\n",
              " 'ing approach for App recommendation [4]. The deep compo-',\n",
              " 'nent similarly uses a MLP on feature embeddings, which has',\n",
              " 'been reported to have strong generalization ability. While',\n",
              " 'their work has focused on incorporating various features',\n",
              " 'of users and items, we target at exploring DNNs for pure',\n",
              " 'collaborative \\x0cltering systems. We show that DNNs are a',\n",
              " 'promising choice for modelling user{item interactions, which',\n",
              " 'to our knowledge has not been investigated before.\\n6. CONCLUSION AND FUTURE WORK',\n",
              " 'In this work, we explored neural network architectures',\n",
              " 'for collaborative \\x0cltering. We devised a general framework',\n",
              " 'NCF and proposed three instantiations | GMF, MLP and',\n",
              " 'NeuMF | that model user{item interactions in di\\x0berent',\n",
              " 'ways. Our framework is simple and generic; it is not limited',\n",
              " 'to the models presented in this paper, but is designed to',\n",
              " 'serve as a guideline for developing deep learning methods for',\n",
              " 'recommendation. This work complements the mainstream',\n",
              " 'shallow models for collaborative \\x0cltering, opening up a new',\n",
              " 'avenue of research possibilities for recommendation based\\non deep learning.',\n",
              " 'on deep learning.\\nIn future, we will study pairwise learners for NCF mod-',\n",
              " 'els and extend NCF to model auxiliary information, such',\n",
              " 'as user reviews [11], knowledge bases [45], and temporal sig-',\n",
              " 'nals [1]. While existing personalization models have primar-',\n",
              " 'ily focused on individuals, it is interesting to develop models',\n",
              " 'for groups of users, which help the decision-making for social',\n",
              " 'groups [15, 42]. Moreover, we are particularly interested in',\n",
              " 'building recommender systems for multi-media items, an in-',\n",
              " 'teresting task but has received relatively less scrutiny in the',\n",
              " 'recommendation community [3]. Multi-media items, such as',\n",
              " 'images and videos, contain much richer visual semantics [16,',\n",
              " \"41] that can re\\rect users' interest. To build a multi-media\",\n",
              " 'recommender system, we need to develop e\\x0bective methods',\n",
              " 'to learn from multi-view and multi-modal data [13, 40]. An-',\n",
              " 'other emerging direction is to explore the potential of recur-',\n",
              " 'rent neural networks and hashing methods [46] for providing\\ne\\x0ecient online recommendation [14, 1].',\n",
              " 'Acknowledgement\\nThe authors thank the anonymous reviewers for their valu-',\n",
              " \"able comments, which are bene\\x0ccial to the authors' thoughts\",\n",
              " 'on recommendation systems and the revision of the paper.\\n7. REFERENCES',\n",
              " '7. REFERENCES\\n[1] I. Bayer, X. He, B. Kanagal, and S. Rendle. A generic',\n",
              " 'coordinate descent framework for learning from implicit\\nfeedback. In WWW , 2017.',\n",
              " '[2] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and',\n",
              " 'O. Yakhnenko. Translating embeddings for modeling',\n",
              " 'multi-relational data. In NIPS , pages 2787{2795, 2013.',\n",
              " '[3] T. Chen, X. He, and M.-Y. Kan. Context-aware image',\n",
              " 'tweet modelling and recommendation. In MM, pages',\n",
              " '1018{1027, 2016.[4] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra,',\n",
              " 'H. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir,',\n",
              " 'et al. Wide & deep learning for recommender systems.\\narXiv preprint arXiv:1606.07792 , 2016.',\n",
              " '[5] R. Collobert and J. Weston. A uni\\x0ced architecture for',\n",
              " 'natural language processing: Deep neural networks with',\n",
              " 'multitask learning. In ICML , pages 160{167, 2008.',\n",
              " '[6] A. M. Elkahky, Y. Song, and X. He. A multi-view deep',\n",
              " 'learning approach for cross domain user modeling in',\n",
              " 'recommendation systems. In WWW , pages 278{288, 2015.',\n",
              " '[7] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol,',\n",
              " 'P. Vincent, and S. Bengio. Why does unsupervised\\npre-training help deep learning? Journal of Machine',\n",
              " 'Learning Research , 11:625{660, 2010.\\n[8] X. Geng, H. Zhang, J. Bian, and T.-S. Chua. Learning',\n",
              " 'image and user features for recommendation in social\\nnetworks. In ICCV , pages 4274{4282, 2015.',\n",
              " '[9] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse recti\\x0cer',\n",
              " 'neural networks. In AISTATS , pages 315{323, 2011.',\n",
              " '[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual',\n",
              " 'learning for image recognition. In CVPR , 2016.',\n",
              " '[11] X. He, T. Chen, M.-Y. Kan, and X. Chen. TriRank:',\n",
              " 'Review-aware explainable recommendation by modeling\\naspects. In CIKM , pages 1661{1670, 2015.',\n",
              " '[12] X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama.',\n",
              " 'Predicting the popularity of web 2.0 items based on user\\ncomments. In SIGIR , pages 233{242, 2014.',\n",
              " '[13] X. He, M.-Y. Kan, P. Xie, and X. Chen. Comment-based',\n",
              " 'multi-view clustering of web 2.0 items. In WWW , pages\\n771{782, 2014.',\n",
              " '771{782, 2014.\\n[14] X. He, H. Zhang, M.-Y. Kan, and T.-S. Chua. Fast matrix',\n",
              " 'factorization for online recommendation with implicit\\nfeedback. In SIGIR , pages 549{558, 2016.',\n",
              " '[15] R. Hong, Z. Hu, L. Liu, M. Wang, S. Yan, and Q. Tian.',\n",
              " 'Understanding blooming human groups in social networks.',\n",
              " 'IEEE Transactions on Multimedia , 17(11):1980{1988, 2015.',\n",
              " '[16] R. Hong, Y. Yang, M. Wang, and X. S. Hua. Learning',\n",
              " 'visual semantic relationships for e\\x0ecient visual retrieval.',\n",
              " 'IEEE Transactions on Big Data , 1(4):152{161, 2015.',\n",
              " '[17] K. Hornik, M. Stinchcombe, and H. White. Multilayer',\n",
              " 'feedforward networks are universal approximators. Neural\\nNetworks , 2(5):359{366, 1989.',\n",
              " '[18] L. Hu, A. Sun, and Y. Liu. Your neighbors a\\x0bect your',\n",
              " 'ratings: On geographical neighborhood in\\ruence to rating\\nprediction. In SIGIR , pages 345{354, 2014.',\n",
              " '[19] Y. Hu, Y. Koren, and C. Volinsky. Collaborative \\x0cltering',\n",
              " 'for implicit feedback datasets. In ICDM , pages 263{272,\\n2008.',\n",
              " '2008.\\n[20] D. Kingma and J. Ba. Adam: A method for stochastic',\n",
              " 'optimization. In ICLR , pages 1{15, 2014.\\n[21] Y. Koren. Factorization meets the neighborhood: A',\n",
              " 'multifaceted collaborative \\x0cltering model. In KDD , pages\\n426{434, 2008.',\n",
              " '426{434, 2008.\\n[22] S. Li, J. Kawale, and Y. Fu. Deep collaborative \\x0cltering via',\n",
              " 'marginalized denoising auto-encoder. In CIKM , pages\\n811{820, 2015.',\n",
              " '811{820, 2015.\\n[23] D. Liang, L. Charlin, J. McInerney, and D. M. Blei.',\n",
              " 'Modeling user exposure in recommendation. In WWW ,\\npages 951{961, 2016.',\n",
              " 'pages 951{961, 2016.\\n[24] M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich. A',\n",
              " 'review of relational machine learning for knowledge graphs.',\n",
              " 'Proceedings of the IEEE , 104:11{33, 2016.',\n",
              " '[25] X. Ning and G. Karypis. Slim: Sparse linear methods for',\n",
              " 'top-n recommender systems. In ICDM , pages 497{506,\\n2011.',\n",
              " '2011.\\n[26] S. Rendle. Factorization machines. In ICDM , pages\\n995{1000, 2010.',\n",
              " '995{1000, 2010.\\n[27] S. Rendle, C. Freudenthaler, Z. Gantner, and',\n",
              " 'L. Schmidt-Thieme. Bpr: Bayesian personalized ranking',\n",
              " 'from implicit feedback. In UAI, pages 452{461, 2009.',\n",
              " '[28] S. Rendle, Z. Gantner, C. Freudenthaler, andL. Schmidt-Thieme. Fast context-aware recommendations',\n",
              " 'with factorization machines. In SIGIR , pages 635{644,\\n2011.',\n",
              " '2011.\\n[29] R. Salakhutdinov and A. Mnih. Probabilistic matrix',\n",
              " 'factorization. In NIPS , pages 1{8, 2008.\\n[30] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted',\n",
              " 'boltzmann machines for collaborative \\x0cltering. In ICDM ,\\npages 791{798, 2007.',\n",
              " 'pages 791{798, 2007.\\n[31] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.',\n",
              " 'Item-based collaborative \\x0cltering recommendation\\nalgorithms. In WWW , pages 285{295, 2001.',\n",
              " '[32] S. Sedhain, A. K. Menon, S. Sanner, and L. Xie. Autorec:',\n",
              " 'Autoencoders meet collaborative \\x0cltering. In WWW , pages\\n111{112, 2015.',\n",
              " '111{112, 2015.\\n[33] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning',\n",
              " 'with neural tensor networks for knowledge base completion.\\nInNIPS , pages 926{934, 2013.',\n",
              " '[34] N. Srivastava and R. R. Salakhutdinov. Multimodal',\n",
              " 'learning with deep boltzmann machines. In NIPS , pages\\n2222{2230, 2012.',\n",
              " '2222{2230, 2012.\\n[35] F. Strub and J. Mary. Collaborative \\x0cltering with stacked',\n",
              " 'denoising autoencoders and sparse inputs. In NIPS\\nWorkshop on Machine Learning for eCommerce , 2015.',\n",
              " '[36] T. T. Truyen, D. Q. Phung, and S. Venkatesh. Ordinal',\n",
              " 'boltzmann machines for collaborative \\x0cltering. In UAI,\\npages 548{556, 2009.',\n",
              " 'pages 548{556, 2009.\\n[37] A. Van den Oord, S. Dieleman, and B. Schrauwen. Deep',\n",
              " 'content-based music recommendation. In NIPS , pages\\n2643{2651, 2013.',\n",
              " '2643{2651, 2013.\\n[38] H. Wang, N. Wang, and D.-Y. Yeung. Collaborative deep',\n",
              " 'learning for recommender systems. In KDD , pages\\n1235{1244, 2015.',\n",
              " '1235{1244, 2015.\\n[39] M. Wang, W. Fu, S. Hao, D. Tao, and X. Wu. Scalable',\n",
              " 'semi-supervised learning by e\\x0ecient anchor graph',\n",
              " 'regularization. IEEE Transactions on Knowledge and Data\\nEngineering , 28(7):1864{1877, 2016.',\n",
              " '[40] M. Wang, H. Li, D. Tao, K. Lu, and X. Wu. Multimodal',\n",
              " 'graph-based reranking for web image search. IEEE',\n",
              " 'Transactions on Image Processing , 21(11):4649{4661, 2012.',\n",
              " '[41] M. Wang, X. Liu, and X. Wu. Visual classi\\x0ccation by l1',\n",
              " 'hypergraph modeling. IEEE Transactions on Knowledge\\nand Data Engineering , 27(9):2564{2574, 2015.',\n",
              " '[42] X. Wang, L. Nie, X. Song, D. Zhang, and T.-S. Chua.',\n",
              " 'Unifying virtual and physical worlds: Learning towards',\n",
              " 'local and global consistency. ACM Transactions on\\nInformation Systems , 2017.',\n",
              " '[43] X. Wang and Y. Wang. Improving content-based and',\n",
              " 'hybrid music recommendation using deep learning. In MM,\\npages 627{636, 2014.',\n",
              " 'pages 627{636, 2014.\\n[44] Y. Wu, C. DuBois, A. X. Zheng, and M. Ester.',\n",
              " 'Collaborative denoising auto-encoders for top-n\\nrecommender systems. In WSDM , pages 153{162, 2016.',\n",
              " '[45] F. Zhang, N. J. Yuan, D. Lian, X. Xie, and W.-Y. Ma.',\n",
              " 'Collaborative knowledge base embedding for recommender\\nsystems. In KDD , pages 353{362, 2016.',\n",
              " '[46] H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S.',\n",
              " 'Chua. Discrete collaborative \\x0cltering. In SIGIR , pages\\n325{334, 2016.',\n",
              " '325{334, 2016.\\n[47] H. Zhang, Y. Yang, H. Luan, S. Yang, and T.-S. Chua.',\n",
              " 'Start from scratch: Towards automatically identifying,',\n",
              " 'modeling, and naming visual attributes. In MM, pages\\n187{196, 2014.',\n",
              " '187{196, 2014.\\n[48] Y. Zheng, B. Tang, W. Ding, and H. Zhou. A neural',\n",
              " 'autoregressive approach to collaborative \\x0cltering. In ICML ,\\npages 764{773, 2016.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add your text to astradb which will first embed the text and then add it to the database\n",
        "astra_db_vector_store.add_texts(splitted_text)\n",
        "astra_vector_store = VectorStoreIndexWrapper(vectorstore = astra_db_vector_store)\n",
        "print(\"number of added texts are {}\".format(len(splitted_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2ofiIxoovN2",
        "outputId": "a18cbe19-39ef-4de6-dd0d-8c109ec849df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of added texts are 882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question your database\n",
        "\n",
        "question = \"what is the main topic of the research paper?\"\n",
        "answer = astra_vector_store.query(question,llm = OPENAI_llm).strip()\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb3aBxrGqmg3",
        "outputId": "f0205e7c-446e-4308-c4a2-84134694778c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main topic of the research paper is the utilization of research efforts and enhancing research problems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Muvbc-4TnrWY"
      }
    }
  ]
}
